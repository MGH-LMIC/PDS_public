{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVE UPDATES TO THE DICTIONARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Diseases, Anatomy, Devices, Procedures\n",
    "def save_dicts():\n",
    "    pd.DataFrame(vis_dis_list, columns = ['name']).to_csv(\"data/csvs/radlex_vis_dis.csv\")\n",
    "    pd.DataFrame(anatomy_list, columns = ['name']).to_csv(\"data/csvs/radlex_anatomy.csv\")\n",
    "    pd.DataFrame(device_list, columns = ['name']).to_csv(\"data/csvs/radlex_devices.csv\")\n",
    "    pd.DataFrame(procedure_list, columns = ['name']).to_csv(\"data/csvs/radlex_procedures.csv\")\n",
    "\n",
    "    pd.DataFrame(locations, columns = ['name']).to_csv(\"data/csvs/radlex_location_list.csv\")\n",
    "    pd.DataFrame(descriptors, columns = ['name']).to_csv(\"data/csvs/radlex_descriptor_list.csv\")\n",
    "\n",
    "    pd.DataFrame(change_list, columns = ['name']).to_csv('data/csvs/change_list.csv')\n",
    "    pd.DataFrame(normal_list, columns = ['name']).to_csv('data/csvs/normal_list.csv')\n",
    "    pd.DataFrame(degree_list, columns = ['name']).to_csv('data/csvs/degree_list.csv')\n",
    "\n",
    "    pd.DataFrame(high_hedges, columns = ['name']).to_csv(\"data/csvs/high_hedges.csv\")\n",
    "    pd.DataFrame(med_hedges, columns = ['name']).to_csv(\"data/csvs/med_hedges.csv\")\n",
    "    pd.DataFrame(low_hedges, columns = ['name']).to_csv(\"data/csvs/low_hedges.csv\")\n",
    "    pd.DataFrame(post_hedge_list, columns = ['name']).to_csv(\"data/csvs/post_hedge_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_data_dir = \"/mnt/nfs/projects/cxr/nlp/data\"\n",
    "cxr_rpt_dir = \"/mnt/nfs/projects/cxr/reports/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO need to get a list of accnums that contain PA views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open (\"{}/pickles/term_rules.p\".format(nlp_data_dir), 'rb') as f:\n",
    "    term_rules = pickle.load(f)\n",
    "with open(\"{}/pickles/neg_rules.p\".format(nlp_data_dir), \"rb\") as f:\n",
    "    neg_rules = pickle.load(f)\n",
    "with open(\"{}/pickles/prev_rules.p\".format(nlp_data_dir), \"rb\") as f:\n",
    "    prev_rules = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis_dis_list = pd.read_csv(\"{}/csvs/radlex_vis_dis.csv\".format(nlp_data_dir)).name.tolist()\n",
    "anatomy_list = pd.read_csv(\"{}/csvs/radlex_anatomy.csv\".format(nlp_data_dir)).name.tolist()\n",
    "device_list = pd.read_csv(\"{}/csvs/radlex_devices.csv\".format(nlp_data_dir)).name.tolist()\n",
    "procedure_list = pd.read_csv('{}/csvs/radlex_procedures.csv'.format(nlp_data_dir)).name.tolist()\n",
    "\n",
    "change_list = pd.read_csv('{}/csvs/change_list.csv'.format(nlp_data_dir)).name.tolist()\n",
    "normal_list = pd.read_csv('{}/csvs/normal_list.csv'.format(nlp_data_dir)).name.tolist()\n",
    "degree_list = pd.read_csv('{}/csvs/degree_list.csv'.format(nlp_data_dir)).name.tolist()\n",
    "\n",
    "locations = pd.read_csv(\"{}/csvs/radlex_location_list.csv\".format(nlp_data_dir)).name.tolist()\n",
    "descriptors = pd.read_csv(\"{}/csvs/radlex_descriptor_list.csv\".format(nlp_data_dir)).name.tolist()\n",
    "post_hedge_list = pd.read_csv(\"{}/csvs/post_hedge_list.csv\".format(nlp_data_dir)).name.tolist()\n",
    "high_hedges = pd.read_csv(\"{}/csvs/high_hedges.csv\".format(nlp_data_dir)).name.tolist()\n",
    "med_hedges = pd.read_csv(\"{}/csvs/med_hedges.csv\".format(nlp_data_dir)).name.tolist()\n",
    "low_hedges = pd.read_csv(\"{}/csvs/low_hedges.csv\".format(nlp_data_dir)).name.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_list(lst):\n",
    "    lengths = [len(item) for item in lst]\n",
    "    lst_df = pd.Series(lengths, index = lst) #, columns = ['len'])\n",
    "    lst_df = lst_df.sort_values(ascending = False)\n",
    "    return lst_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set Up Hedge Scoring System & Check That No Hedges Are Placed in Multiple Categories\n",
    "for h in high_hedges:\n",
    "    for m in med_hedges:\n",
    "        for l in low_hedges:\n",
    "            if h == m or h == l or l == m:\n",
    "                print(\"ERROR: redundancy found\", h, m, l)\n",
    "\n",
    "hedge_list = list(set(high_hedges + low_hedges + med_hedges ))         \n",
    "\n",
    "hedge_dict = {}\n",
    "for word in low_hedges:\n",
    "    hedge_dict[word] = 'low'\n",
    "for word in med_hedges:\n",
    "    hedge_dict[word] = 'medium'\n",
    "for word in high_hedges:\n",
    "    hedge_dict[word] = 'high'\n",
    "    \n",
    "hedge_scores = { 'low': 3,'medium': 2,'high': 1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# radnet = pd.read_excel(\"data/csvs/radnet_norm_parsed.xlsx\")\n",
    "# df = pd.read_csv(\"{}/radnet_cxr_100K_reports_parsed.csv\".format(cxr_rpt_dir), delimiter=\"|\", dtype=str)\n",
    "df = pd.read_csv(\"{}/radnet_cxr_784K_reports_parsed.csv\".format(cxr_rpt_dir), delimiter=\"|\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784248, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mrn', 'accnum', 'EXAM', 'HISTORY', 'TECHNIQUE', 'COMPARISON',\n",
       "       'FINDINGS', 'IMPRESSION', 'Conclusion', 'report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_sents(findings):\n",
    "    if type(findings) == None or type(findings) == float:\n",
    "        return []\n",
    "    else:\n",
    "        sentences = nltk.tokenize.sent_tokenize(findings)\n",
    "        sentences = [sent.lower() for sent in sentences]\n",
    "        sentences = [sent.split(\"   \") for sent in sentences]\n",
    "        sentences = [sent for sents in sentences for sent in sents]\n",
    "        sentences = [re.sub('\\d+?/\\d+?/\\d{2,}', '', sent) for sent in sentences]\n",
    "        sentences = [sent.replace(\"/\", \" \").replace(\"\\n\", \" \") for sent in sentences]\n",
    "        sentences = [sent.replace(\"chronic obstructive pulmonary disease\", \"copd\") for sent in sentences]\n",
    "        sentences = [sent.replace(\"coronary artery bypass graft\", \"cabg\") for sent in sentences]\n",
    "        sentences = [sent.replace(\"coronary bypass surgery\", \"cabg\") for sent in sentences]\n",
    "        sentences = [sent.replace(\"tb\", \"tuberculosis\") for sent in sentences]\n",
    "        sentences = [sent.replace(\"cp\", \"costophrenic\") for sent in sentences]\n",
    "        sentences = [sent.replace(\".\", \" \") for sent in sentences]\n",
    "        return sentences\n",
    "\n",
    "def get_sents(df):\n",
    "    imps, finds = [], []\n",
    "    for i in range(len(df)):\n",
    "        pt = df.iloc[i]\n",
    "        imps.append(pre_process_sents(pt.IMPRESSION))\n",
    "        finds.append(pre_process_sents(pt.FINDINGS))\n",
    "    imps = list(set([sent for sents in imps for sent in sents]))\n",
    "    finds = list(set([sent for sents in finds for sent in sents]))\n",
    "    return imps, finds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClauseSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Split initial list of sentences into a list of sentences by clause'''\n",
    "def split_by_clause(sentence, term_rules):\n",
    "    \n",
    "    '''Subfunction to split up sentence if the word AND is present'''\n",
    "    def split_ands(phrases):\n",
    "        new_phrases = []\n",
    "        for phrase in phrases:\n",
    "            if phrase.count('and') == 1 and \",\" not in phrase:\n",
    "                parts = phrase.split('and')\n",
    "                pos1, pos2 = [token.pos_ for token in nlp(parts[0])], [token.pos_ for token in nlp(parts[1])]\n",
    "                if 'NOUN' in pos1 and 'VERB' in pos1 and 'NOUN' in pos2 and 'VERB' in pos2:  #maybe also 'ADV'\n",
    "                    new_phrases.append(parts[0])\n",
    "                    new_phrases.append(parts[1])\n",
    "                else:\n",
    "                    new_phrases.append(phrase)\n",
    "            else:\n",
    "                new_phrases.append(phrase)\n",
    "        return new_phrases\n",
    "    \n",
    "    '''Subfunction to split up sentence into comma-separated phrases'''\n",
    "    def split_sent_by_comma(sent):\n",
    "        list_start, list_end = 0, 0\n",
    "        comma_indices = [c.start() for c in re.finditer(',', sent)]\n",
    "\n",
    "        lst_indices = []\n",
    "\n",
    "        #if oxford comma\n",
    "        if re.findall(', (((\\w+\\s?){1,2},)\\s)+?(and|or)', sent):\n",
    "            lst_indices = [(c.start(), c.end()) for c in re.finditer(', (((\\w+\\s?){1,2},)\\s)+?(and|or)', sent)]\n",
    "        #if no oxford comma\n",
    "        #elif re.findall(', ((\\w+\\s?){1,2},)+?(\\s\\w+)\\s(and|or)', sent):\n",
    "        elif re.findall('((\\w+\\s?){1,2},\\s?)+?(\\s\\w+)+?\\s(and|or)', sent):\n",
    "            #lst_indices = [(c.start(), c.end()) for c in re.finditer(', ((\\w+\\s?){1,2},)+?(\\s\\w+)\\s(and|or)', sent)]\n",
    "            lst_indices = [(c.start(), c.end()) for c in re.finditer('((\\w+\\s?){1,2},\\s?)+?(\\s\\w+)+?\\s(and|or)', sent)]\n",
    "\n",
    "        split_zones = [0]\n",
    "        for j in range(len(lst_indices)):\n",
    "            split_zones = split_zones + [i for i in range(split_zones[-1], lst_indices[j][0])] + [lst_indices[j][1]]\n",
    "        split_zones = split_zones + [i for i in range(split_zones[-1], len(sent))]\n",
    "\n",
    "        to_split = []\n",
    "\n",
    "        for idx in comma_indices:\n",
    "            if idx in split_zones:\n",
    "                to_split.append(idx)\n",
    "\n",
    "        sxns = [sent[i:j] for i, j in zip([0] + to_split, to_split + [len(sent)])]\n",
    "\n",
    "        return sxns\n",
    "\n",
    "    term_pat, clauses = \"\\[TERM\\]\", []\n",
    "\n",
    "    for rule in term_rules:  #check every rule for a clause termination word \n",
    "        reformatRule = re.sub(r'\\s+', '_', rule[0].strip())\n",
    "        sentence = rule[3].sub(' ' + rule[2].strip()   #add in Negation tag to \n",
    "                                  + reformatRule + rule[2].strip() + ' ', sentence)\n",
    "\n",
    "    if re.findall(term_pat, sentence, flags = re.IGNORECASE):   #if termination words exist, split up the phrases by them\n",
    "        phrases = re.split(term_pat, sentence, flags = re.IGNORECASE)\n",
    "        phrases = [\" \".join([word.strip() for word in phrase.split()]) for phrase in phrases if len(phrase.split()) > 1]\n",
    "        phrases = [split_sent_by_comma(phrase) for phrase in phrases]  #Split phrases by comma, except in list case\n",
    "        phrases = [phrase for sub_phrase in phrases for phrase in sub_phrase]\n",
    "        phrases = [re.split(';|:', phrase) for phrase in phrases]\n",
    "        phrases = [phrase for sub_phrase in phrases for phrase in sub_phrase]\n",
    "        phrases = [phrase.split(\"  \") for phrase in phrases]\n",
    "        phrases = [phrase for sub_phrase in phrases for phrase in sub_phrase]\n",
    "        for phrase in phrases:\n",
    "            clauses.append(phrase.lower())\n",
    "    else:  #if no termination words exist, return listicized sentence following other split rules\n",
    "        phrases = split_sent_by_comma(sentence) \n",
    "        phrases = [re.split(';|:', phrase) for phrase in phrases]\n",
    "        phrases = [phrase for sub_phrase in phrases for phrase in sub_phrase]\n",
    "        phrases = [phrase.split(\"  \") for phrase in phrases]\n",
    "        phrases = [phrase for sub_phrase in phrases for phrase in sub_phrase]\n",
    "        phrases = split_ands(phrases)\n",
    "        for phrase in phrases:\n",
    "            if len(phrase) != 0:\n",
    "                clauses.append(phrase.lower())      \n",
    "    return clauses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Classes (Descriptor, ChangeEntity, ExtractedEntity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Descriptor(object):\n",
    "    def __init__(self, name, qualifiers = [], hedges = []):\n",
    "        self.name = name\n",
    "        self.hedge = self.set_hedging(hedges)\n",
    "        self.qualifiers = qualifiers\n",
    "        \n",
    "    def set_hedging(self, hedges):\n",
    "        self.hedge = \", \".join(hedges)\n",
    "    \n",
    "    def describe(self):\n",
    "        if len(self.qualifiers) > 0:\n",
    "            return self.name + \" (\" + \", \".join(self.qualifiers) + \")\"\n",
    "        else:\n",
    "            return self.name\n",
    "        \n",
    "class ChangeEntity(object):\n",
    "    def __init__(self, name, prior_exam_exists = False, location = '', description = []):\n",
    "        self.name = name\n",
    "        self.prior_exam_exists = prior_exam_exists\n",
    "        self.location = location\n",
    "        self.description = description  \n",
    "\n",
    "        \n",
    "class ExtractedEntity(object):\n",
    "    def __init__(self, name, ent_type = '', location = '', hedges = [],\n",
    "                 description = [], \n",
    "                 is_previous = False, is_normal = False, is_negated = False):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self.ent_type = ent_type  #Vis-Disease, Anatomy, Procedure, Device, Change\n",
    "        self.location = location\n",
    "        self.hedges = hedges\n",
    "        self.description = description  #this should hold Descriptor objects\n",
    "        \n",
    "        self.previous = is_previous  #should be 'current' or 'previous'\n",
    "        self.normality = is_normal  #should be 'abnormal' or 'normal'\n",
    "        self.negated = is_negated\n",
    "        \n",
    "        self.hedging = self.set_hedging(self.hedges)\n",
    "        \n",
    "    def describe(self):\n",
    "        return_string = \"\"\n",
    "        for descriptor in self.description:\n",
    "            doc = nlp(descriptor.describe())\n",
    "            tok_pos = [token.pos_ for token in doc]\n",
    "            if 'NOUN' not in tok_pos:\n",
    "            \n",
    "                if descriptor.hedge is None:\n",
    "                    return_string = descriptor.describe() + \", \" + return_string\n",
    "                else:\n",
    "                    #modify this later to match test set format\n",
    "                     return_string = descriptor.describe() + \" (\" + descriptor.hedge + \"), \" + return_string\n",
    "        return return_string.strip(\", \")\n",
    "               \n",
    "    def is_changed(self, change_rules):\n",
    "        change_present = False\n",
    "        \n",
    "        if self.name in change_rules:\n",
    "            change_present = True\n",
    "        for descriptor in self.description:\n",
    "            if descriptor.name in change_rules:\n",
    "                change_present = True\n",
    "        \n",
    "        if not change_present:\n",
    "            return False\n",
    "        elif self.negated:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    def set_hedging(self, hedges):\n",
    "        self.hedging = \", \".join(hedges)\n",
    "        self.hedges = hedges\n",
    "    \n",
    "    def set_hedge_level(self, hedge_dict, hedge_scores):\n",
    "        score = np.sum([hedge_scores[hedge_dict[word]] for word in self.hedges])\n",
    "        if score <= 1:\n",
    "            self.hedge_level = 'high'\n",
    "        elif score >= 3:\n",
    "            self.hedge_level = 'low'\n",
    "        else:\n",
    "            self.hedge_level = 'medium'\n",
    "        return self.hedge_level\n",
    "    \n",
    "    def get_description(self, certainties = True):\n",
    "        if certainties:\n",
    "            return str([str(mod.certainty) + \" \" + str(mod.name) for mod in self.description]).replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "        else:\n",
    "            return str([mod.name for mod in self.description]).replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "        \n",
    "    def output(self):\n",
    "        output = str(self.name)\n",
    "        values = [self.category, self.location, \n",
    "                      self.get_description(), self.certainty]\n",
    "        value_names = ['category', 'location', 'description', 'certainty']\n",
    "        \n",
    "        for i in range(len(values)):\n",
    "            value = values[i]\n",
    "            if value is not '':\n",
    "                output = output + \"     \" + value_names[i].upper() + \": \" + value\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReportExtractor(object):\n",
    "    def __init__(self, \n",
    "                 clause = None, neg_rules = None, prev_rules = None, \n",
    "                 \n",
    "                 vis_dis_list = [], anatomy_list = [], \n",
    "                 procedure_list = [], device_list = [], change_list = [],\n",
    "                 \n",
    "                 locations_list = [], descriptor_list = [], normal_list = [],\n",
    "                 \n",
    "                 hedge_list = [], post_hedge_list = [], \n",
    "                 hedge_dict = {}, hedge_scores = {}, grab = False):\n",
    "        \n",
    "        self.__filler = '_'\n",
    "        self.__clause = clause\n",
    "        self.__clause_doc = nlp(clause)\n",
    "        self.__keys = ['visual_disease', 'anatomy', 'procedure', 'device', 'change']\n",
    "        self.__neg_rules = neg_rules\n",
    "        self.__prev_rules = prev_rules\n",
    "        self.__vis_dis_list = vis_dis_list\n",
    "        self.__anatomy_list = anatomy_list\n",
    "        self.__procedure_list = procedure_list\n",
    "        self.__device_list = device_list\n",
    "        \n",
    "        \n",
    "        self.__joined_rules = []\n",
    "        \n",
    "        for rule_set in [neg_rules, prev_rules]:\n",
    "            if rule_set is not None:\n",
    "                self.__joined_rules = self.__joined_rules + rule_set\n",
    "        \n",
    "        self.__dicts = self.bind_dicts([vis_dis_list, anatomy_list, procedure_list, device_list, change_list])\n",
    "        \n",
    "        self.__neg_list = [rule[0] for rule in neg_rules if rule[3] is not '[PSEU]']\n",
    "        self.__location_list = locations_list\n",
    "        self.__descriptor_list = descriptor_list\n",
    "        self.__normal_list = normal_list\n",
    "        self.__change_list = change_list\n",
    "        \n",
    "        \n",
    "        self.__hedge_list = hedge_list\n",
    "        self.__post_hedges = post_hedge_list\n",
    "        self.__hedge_dict = hedge_dict\n",
    "        self.__hedge_scores = hedge_scores\n",
    "        \n",
    "        self.__grab = grab\n",
    "        return None\n",
    "    \n",
    "    '''MAIN CALL'''\n",
    "    def run_extractor(self):\n",
    "        self.__chunks, self.__tagged_chunks, self.__raw_mapping_dicts, self.__full_chunks_dicts = self.run_chunker() \n",
    "        self.__tagged_clause = self.tag(self.__clause)\n",
    "        self.__chunk_idx_dicts = self.get_indices()\n",
    "        self.__all_entities = self.parse_indices()   #should be a list of ExtractedEntity objects\n",
    "        self.apply_hedges()\n",
    "        return self.clean_output()\n",
    "\n",
    "    \n",
    "    '''INITIALIZING FUNCTIONS'''\n",
    "    def bind_dicts(self, lists):\n",
    "        new_dict, key_idx = {}, 0\n",
    "        for lst in lists:\n",
    "            new_dict[self.__keys[key_idx]] = lst\n",
    "            key_idx += 1\n",
    "        return new_dict\n",
    "    \n",
    "    def tag(self, phrase):\n",
    "        for rule in self.__joined_rules:\n",
    "                reformatRule = re.sub(r'\\s+', self.__filler, rule[0].strip())\n",
    "                phrase = rule[3].sub(' ' + rule[2].strip() + reformatRule + rule[2].strip() + ' ', phrase)\n",
    "        return phrase \n",
    "    \n",
    "    '''MAPPING TO RADLEX FUNCTIONS'''\n",
    "    def map_text_to_radlex(self, text, dictionary):\n",
    "        text = text.strip()\n",
    "        if text in dictionary:\n",
    "            return text, text\n",
    "        \n",
    "        #SUFFIX SUBSTITUTIONS\n",
    "        #plurals\n",
    "        elif text.replace(\"'s\", \"\") in dictionary:\n",
    "            return text, text.replace(\"'s\", \"\")\n",
    "        elif text.replace(\"ies\", \"y\") in dictionary:\n",
    "            return text, text.replace(\"ies\", \"ty\")\n",
    "        elif text.replace(\"es\", \"is\") in dictionary:\n",
    "            return text, text.replace(\"es\", \"is\")\n",
    "        elif text[:-1] in dictionary:\n",
    "            return text, text[:-1]\n",
    "        #-ing/-ed/-ion words all map to -ed (ie, \"hyperinflation of the lungs\")\n",
    "        elif text.replace(\"ing\", \"ed\") in dictionary:\n",
    "            return text, text.replace(\"ing\", \"ion\")\n",
    "        elif text.replace(\"ing\", \"\") in dictionary:  #should catch also spurring/spur\n",
    "            return text, text.replace(\"ing\", \"\")\n",
    "        elif text.replace(\"ring\", \"\") in dictionary:\n",
    "            return text, text.replace(\"ring\", \"\")\n",
    "        elif text.replace(\"ion\", \"ed\") in dictionary:\n",
    "            return text, text.replace(\"ion\", \"ed\")\n",
    "        elif text.replace(\"ative\", \"ed\") in dictionary:\n",
    "            return text, text.replace(\"ative\", \"ed\") \n",
    "        #-atous/-a matches (ie emphysema/emphysematous, atheroma/atheromatous)\n",
    "        elif text.replace(\"tous\", \"\") in dictionary:\n",
    "            return text, text.replace(\"tous\", \"\")\n",
    "        #WHY THIS?\n",
    "        elif text.replace(\"ed\", \"ement\") in dictionary:\n",
    "            return text, text.replace(\"ed\", \"ement\")\n",
    "        #haziness/hazy\n",
    "        elif text.replace(\"iness\", \"y\") in dictionary:\n",
    "            return text, text.replace(\"iness\", \"y\")\n",
    "        #mildly/mild\n",
    "        elif text.replace(\"ly\", \"\") in dictionary:\n",
    "            return text, text.replace(\"ly\", \"\") \n",
    "        #tortuousity/tortuous\n",
    "        elif text.replace(\"ity\", \"\") in dictionary:\n",
    "            return text, text.replace(\"ity\", \"\")\n",
    "        #infiltrate/infiltration\n",
    "        elif text.replace(\"e\", \"ion\") in dictionary:\n",
    "            return text, text.replace(\"e\", \"ion\")\n",
    "        \n",
    "        #AFFIX SUBSTITUTIONS\n",
    "        elif text.replace(\"para\", \"\") in dictionary:\n",
    "            return text, text.replace(\"para\", \"\")\n",
    "        elif text.replace(\"peri\", \"\") in dictionary:\n",
    "            return text, text.replace(\"peri\", \"\")\n",
    "        elif text.replace(\"bi\", \"\") in dictionary:\n",
    "            return text, text.replace(\"bi\", \"\")\n",
    "        \n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    def map_to_radlex(self, chunk, dictionary):\n",
    "        if type(chunk) != str:\n",
    "            text = chunk.text\n",
    "        else:\n",
    "            text = chunk\n",
    "        text_words, word_idx = text.split(), 0\n",
    "\n",
    "        found_mapping = False\n",
    "        \n",
    "        while not found_mapping and word_idx < len(text_words):\n",
    "            raw_text, mapped = self.map_text_to_radlex(text, dictionary) \n",
    "            if raw_text is not None:\n",
    "                 found_mapping = True\n",
    "            else:\n",
    "                word_idx += 1\n",
    "                text = \" \".join(text_words[word_idx:])  \n",
    "        \n",
    "        if word_idx == len(text_words) and not found_mapping:\n",
    "            for word in text_words[:-1]:\n",
    "                if not found_mapping:\n",
    "                    raw_text, mapped = self.map_text_to_radlex(word, dictionary) \n",
    "                    if raw_text is not None:\n",
    "                        found_mapping = True\n",
    "                        \n",
    "        return [raw_text, mapped] \n",
    "    \n",
    "    def remove_submatches(self, matches_to_search):\n",
    "        unique_matches = []\n",
    "        \n",
    "        while len(matches_to_search) > 0:\n",
    "            match1 = max(matches_to_search, key = len)\n",
    "            related_matches = [match1]\n",
    "            matches_to_search.remove(match1)     \n",
    "            for match2 in matches_to_search:\n",
    "                if match2 in match1:\n",
    "                    related_matches.append(match2)\n",
    "            unique_matches.append(max(related_matches, key = len))\n",
    "            for match in related_matches:\n",
    "                if match in matches_to_search:\n",
    "                    matches_to_search.remove(match)              \n",
    "        return unique_matches\n",
    "    \n",
    "    def map_modifiers(self, possible_modifiers):\n",
    "        mod_descriptors = []\n",
    "        is_normal = False\n",
    "\n",
    "        for modifier in possible_modifiers:\n",
    "            #print(\"IN MAP MODIFIERS, mapping\", modifier)\n",
    "            descripts = self.map_to_radlex(modifier, self.__descriptor_list)\n",
    "            normals = self.map_to_radlex(modifier, self.__normal_list)\n",
    "            \n",
    "            if descripts[0] is not None:\n",
    "                #print(\"found a descriptor, \", descripts[0])\n",
    "                descr_name, adverbs = descripts[0], []\n",
    "                descr_token = [token for token in self.__clause_doc if token.text == descripts[0]][0]\n",
    "                if descr_token.pos_ == 'VERB' or descr_token.pos_ == 'ADJ':\n",
    "                    adverbs = [child.text for child in descr_token.children if child.pos_ == 'ADV']\n",
    "                mod_descriptors.append(Descriptor(name = descr_name, qualifiers = adverbs))\n",
    "            if normals[0] is not None:\n",
    "                is_normal = True \n",
    "        return is_normal, mod_descriptors\n",
    "    \n",
    "    def map_locations(self, possible_locations): \n",
    "        #print(\"in MAP LOCATIONS, considering \", possible_locations)\n",
    "        locations = []\n",
    "        for location in possible_locations:\n",
    "            locs = self.map_to_radlex(location, self.__location_list)\n",
    "            \n",
    "            if locs[0] is not None:\n",
    "                locations.append(locs[0])\n",
    "        return locations\n",
    "\n",
    "    \n",
    "    def run_chunker(self):\n",
    "        #KEYS ORDER: 'visual-disease', 'anatomy', 'procedure', 'device', 'change'\n",
    "        num_keys = len(self.__keys)\n",
    "        chunk_lists, tagged_chunk_lists = [[] for i in range(num_keys)], [[] for i in range(num_keys)]\n",
    "        mapping_dicts, full_chunk_dicts =  [{} for i in range(num_keys)], [{} for i in range(num_keys)]\n",
    "\n",
    "        chunks = list(set([chunk for chunk in self.__clause_doc.noun_chunks] + [token for token in self.__clause_doc if token.pos_ == 'VERB' or token.pos_ == 'NOUN' or token.pos_ == 'ADJ']))\n",
    "        narrowed_chunk_texts = self.remove_submatches([chunk.text for chunk in chunks])\n",
    "        chunks = [chunk for chunk in chunks if chunk.text in narrowed_chunk_texts]\n",
    "        self.__spacy_chunks = chunks\n",
    "        \n",
    "        #Sort chunks and store mappings\n",
    "        for chunk_list_idx in range(num_keys):\n",
    "            for chunk in chunks:\n",
    "                raw, mapped = self.map_to_radlex(chunk, self.__dicts[self.__keys[chunk_list_idx]])\n",
    "                \n",
    "                if raw is not None and raw not in chunk_lists[chunk_list_idx]:\n",
    "                    full_chunk_dicts[chunk_list_idx][raw] = chunk \n",
    "                    chunk_lists[chunk_list_idx].append(raw)\n",
    "                    mapping_dicts[chunk_list_idx][raw] = mapped\n",
    "            \n",
    "            chunk_lists[chunk_list_idx] = self.remove_submatches(chunk_lists[chunk_list_idx])\n",
    "        \n",
    "        #caveat\n",
    "        if \"lymph node\" in \" \".join(chunk_lists[1]):\n",
    "            replace_vis_dis_list = []\n",
    "            for chunk in chunk_lists[0]:  #vis_dis \n",
    "                if \"node\" not in chunk:\n",
    "                    replace_vis_dis_list.append(chunk)\n",
    "            chunk_lists[0] = replace_vis_dis_list\n",
    "        \n",
    "        #Tag the raw_text of the chunks with rules (allow for proper indexing later)\n",
    "        tagged_chunk_list_idx = 0\n",
    "        for chunk_list in chunk_lists:\n",
    "            for raw in chunk_list:\n",
    "                tagged_chunk_lists[tagged_chunk_list_idx].append(self.tag(raw))\n",
    "            tagged_chunk_list_idx += 1\n",
    "        \n",
    "        return chunk_lists, tagged_chunk_lists, mapping_dicts, full_chunk_dicts\n",
    "    \n",
    "    \n",
    "    def get_indices(self):\n",
    "        chunk_idx_dicts = []\n",
    "        for lst in self.__tagged_chunks:\n",
    "            chunk_idx_dict = {}\n",
    "            for tagged_chunk in lst:\n",
    "                chunk_idx_dict[tagged_chunk] = self.__tagged_clause.index(tagged_chunk)\n",
    "            chunk_idx_dicts.append(chunk_idx_dict)\n",
    "        return chunk_idx_dicts\n",
    "    \n",
    "\n",
    "    '''CHECK NEGATIONS AND PREVIOUS STATUS'''\n",
    "    def check_change_negation(self, raw_chunk):\n",
    "        is_negated = False\n",
    "        \n",
    "        for token in self.__clause_doc:\n",
    "            \n",
    "            if token.text in raw_chunk:\n",
    "                if token.pos_ == 'NOUN':\n",
    "                    to_check = [child.text for child in token.children]   \n",
    "                else:\n",
    "                    to_check = [child.text for anc in [anc for anc in token.ancestors] for child in anc.children]\n",
    "                    \n",
    "                for word in self.__neg_list:\n",
    "                        if word in to_check:\n",
    "                            is_negated = True\n",
    "        return is_negated\n",
    "        \n",
    "    def check_negation(self, chunk_idx):\n",
    "        #Find the indices of the pre-negation, post-negation flags\n",
    "        clause_words, preneg_idxs, postneg_idxs = self.__tagged_clause.split(), [], []\n",
    "        \n",
    "        for word in clause_words:\n",
    "            if re.findall('\\[PREN\\]|\\[PREP\\]', word):\n",
    "                preneg_idxs.append(self.__tagged_clause.index(word))\n",
    "            if re.findall('\\[POST\\]|\\[POSP\\]', word):\n",
    "                postneg_idxs.append(self.__tagged_clause.index(word))\n",
    "              \n",
    "        #return false if no negation tags present\n",
    "        if len(preneg_idxs) == 0 and len(postneg_idxs) == 0:\n",
    "            return False\n",
    "        \n",
    "        #extract only those tags that are before or after the chunk idx\n",
    "        preneg_idxs = [neg_idx for neg_idx in preneg_idxs if neg_idx < chunk_idx]\n",
    "        postneg_idxs = [neg_idx for neg_idx in postneg_idxs if neg_idx > chunk_idx]\n",
    "        \n",
    "        #Set pre-neg/post-neg/previous indices, handling multiple negations\n",
    "        if len(preneg_idxs) % 2 == 0:\n",
    "            is_pre_negated = False\n",
    "        else:\n",
    "            is_pre_negated = True\n",
    "        \n",
    "        if len(postneg_idxs) % 2 == 0:\n",
    "            is_post_negated = False\n",
    "        else:\n",
    "            is_post_negated = True\n",
    "            \n",
    "        neg_sum = is_pre_negated + is_post_negated\n",
    "        \n",
    "        if neg_sum % 2 == 0: #if not pre or post negated, or if both pre and post negated\n",
    "            return False\n",
    "        if neg_sum == 1:\n",
    "            return True \n",
    "    \n",
    "    def check_previous(self, chunk_idx):\n",
    "        clause_words, prev_idxs = self.__tagged_clause.split(), []\n",
    "        \n",
    "        for word in clause_words:\n",
    "            if re.findall('\\[PREV\\]', word):\n",
    "                prev_idxs.append(self.__tagged_clause.index(word))\n",
    "        if len(prev_idxs) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            prev_idx = prev_idxs[0]\n",
    "            if prev_idx < chunk_idx:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "###DEBUG LOCATIONS\n",
    "    '''PARSING ENTITIES FUNCTIONS'''\n",
    "    def get_modifiers(self, token, location = False):\n",
    "        \n",
    "        children = [child.text for child in token.children if child.pos_ in ['ADJ', 'ADV', 'NOUN', 'VERB']]\n",
    "        ancestors = [anc.text for anc in token.ancestors if anc.pos_ in ['VERB','ADJ', 'ADV', 'NOUN']]\n",
    "        additionals = [tok.text for tok in self.__clause_doc if (tok.dep_ in ['conj', 'acomp', 'xcomp']) and (token in [child for child in tok.children] or token in [anc for anc in tok.ancestors])]\n",
    "        \n",
    "        dets = [tok.text for tok in self.__clause_doc if tok.dep_ is 'det' and (token in [anc for anc in tok.ancestors])]\n",
    "\n",
    "        if location:\n",
    "            lost_adjs = [tok.text for tok in self.__clause_doc if tok.pos_ == 'ADJ' and token in [anc for anc in tok.ancestors]]\n",
    "        else:\n",
    "            lost_adjs = []\n",
    "        possible_modifiers = list(set(children + ancestors + additionals + dets + lost_adjs))\n",
    "        \n",
    "       # print(\"IN GET MODIIFERS, possible mods are \", possible_modifiers)\n",
    "        \n",
    "        if not location:\n",
    "            narrowed_modifiers = []\n",
    "            for modifier in possible_modifiers:\n",
    "                mod_token = [tok for tok in self.__clause_doc if tok.text == modifier][0]\n",
    "                mod_dependents = [child for child in mod_token.children] + [anc for anc in mod_token.ancestors]\n",
    "                if token in mod_dependents:\n",
    "                    narrowed_modifiers.append(modifier)\n",
    "            #print(\"not location, returning a narrowed version\")  #DEBUG\n",
    "            return narrowed_modifiers\n",
    "        else:\n",
    "            return possible_modifiers\n",
    "    \n",
    "    def get_location(self, ent):\n",
    "        \n",
    "        \n",
    "        possible_modifiers = []\n",
    "        for token in self.__clause_doc:\n",
    "            if token.text in ent:\n",
    "                possible_modifiers = possible_modifiers + self.get_modifiers(token, location = True)          \n",
    "        location = self.map_locations(possible_modifiers)\n",
    "        \n",
    "        #Check anatomy chunks for mapping\n",
    "        anatomy_chunks = self.__chunks[1]\n",
    "       # print(\"IN GET LOCATION,   FULL LIST OF ANATOMY CHUNKS ARE: \", anatomy_chunks)\n",
    "        for anatomy in anatomy_chunks:\n",
    "            anatomy_chunk = self.__full_chunks_dicts[1][anatomy]\n",
    "           # print(\"IN GET LOCATION, considering chunk\", anatomy, \"with full chunk\", anatomy_chunk, \"\\n\")\n",
    "        \n",
    "            if type(anatomy_chunk) == spacy.tokens.span.Span:\n",
    "                ancestors = [anc.text for anc in anatomy_chunk.root.ancestors]\n",
    "                children = [child.text for child in anatomy_chunk.root.children]\n",
    "                \n",
    "            else:\n",
    "                ancestors = [anc.text for anc in anatomy_chunk.ancestors]\n",
    "                children = [child.text for child in anatomy_chunk.children]\n",
    "            \n",
    "            #print(\"considering ancestors and children\", ancestors, children)\n",
    "            retain_anatomy = []\n",
    "            for word in ent.split(\" \"):\n",
    "                #print(\"looking at word\", word)\n",
    "                #print(\"it should match \", ancestors + children)\n",
    "                if word in ancestors + children:\n",
    "                    words = anatomy_chunk.text.split()\n",
    "                    refined_words = []\n",
    "                    for word in words:\n",
    "                       # print(\"LOOKING AT WORD\", word)\n",
    "                        if word not in [\"the\", \"a\"]:\n",
    "                            if self.map_to_radlex(word, self.__location_list)[0] is not None:\n",
    "                                refined_words.append(word)\n",
    "                            elif self.map_to_radlex(word, self.__anatomy_list)[0] is not None:\n",
    "                                refined_words.append(word)\n",
    "                    location.append(\" \".join(refined_words))\n",
    "        location = list(set(location))\n",
    "        location = self.remove_submatches(location)\n",
    "        return location\n",
    "\n",
    "    \n",
    "    def parse_entity(self, idx, ent_type):\n",
    "        entity_chunks, raw_chunks = self.__tagged_chunks[idx], self.__chunks[idx]\n",
    "        chunk_idx_dict = self.__chunk_idx_dicts[idx]\n",
    "        entity_ents, chunk_list_idx  = [], 0\n",
    "        \n",
    "        if ent_type == 'anatomy':\n",
    "            location = True\n",
    "        else: \n",
    "            location = False\n",
    "        \n",
    "        while chunk_list_idx < len(entity_chunks):\n",
    "            tagged_chunk, raw_chunk = entity_chunks[chunk_list_idx], raw_chunks[chunk_list_idx]\n",
    "            chunk_idx = chunk_idx_dict[tagged_chunk]\n",
    "            \n",
    "            is_previous = self.check_previous(chunk_idx)\n",
    "            is_negated = self.check_negation(chunk_idx)\n",
    "            \n",
    "            possible_modifiers = []\n",
    "            for token in self.__clause_doc:\n",
    "                if token.text in raw_chunk:\n",
    "                    possible_modifiers = possible_modifiers + self.get_modifiers(token, location)\n",
    "                \n",
    "            is_normal, mod_descriptors = self.map_modifiers(list(set(possible_modifiers)))\n",
    "            #print(\"IN PARSE_ENTITY: Possible modifiers are\", possible_modifiers,\n",
    "                  #\"selected descriptors are\", [descr.name for descr in mod_descriptors])\n",
    "            \n",
    "            if not (ent_type == 'anatomy' and len(mod_descriptors) == 0):\n",
    "                if ent_type == 'anatomy':\n",
    "                    \n",
    "                    potential_location = self.__full_chunks_dicts[1][raw_chunk].text\n",
    "                    refined_words = []\n",
    "                    for word in potential_location.split():\n",
    "                        if word not in [\"the\", \"a\"]:\n",
    "                            if self.map_to_radlex(word, self.__location_list)[0] is not None:\n",
    "                                refined_words.append(word)\n",
    "                            elif self.map_to_radlex(word, self.__anatomy_list)[0] is not None:\n",
    "                                refined_words.append(word)\n",
    "                    anat_loc = \" \".join(refined_words)\n",
    "                    \n",
    "                    \n",
    "                    ent_name = raw_chunk + \" (\" + \", \".join([descriptor.describe() for descriptor in mod_descriptors]) + \")\"\n",
    "                    location = [anat_loc] #self.get_location(raw_chunk)\n",
    "                else: \n",
    "                    ent_name = raw_chunk\n",
    "                    location = self.get_location(raw_chunk)\n",
    "                if ent_type == 'change':\n",
    "                    is_negated = self.check_change_negation(raw_chunk)\n",
    "                    \n",
    "                entity_ents.append(ExtractedEntity(name = ent_name, ent_type = ent_type,\n",
    "                                                    location = \", \".join(location),\n",
    "                                                    description = mod_descriptors,\n",
    "                                                    is_previous = is_previous,\n",
    "                                                    is_negated = is_negated, is_normal = is_normal))\n",
    "            chunk_list_idx += 1\n",
    "        \n",
    "        return entity_ents\n",
    "    \n",
    "    def parse_indices(self):\n",
    "        entity_lsts = [self.parse_entity(i, self.__keys[i]) for i in range(len(self.__keys))]\n",
    "        return [ent for lst in entity_lsts for ent in lst]\n",
    "\n",
    "    '''HEDGING CODE''' \n",
    "    def apply_hedges(self):\n",
    "        hedges = [phrase for phrase in self.__hedge_list if phrase in self.__clause]\n",
    "        \n",
    "        doc = nlp(self.__clause) \n",
    "        if len(hedges) > 0:\n",
    "            \n",
    "            for entity in self.__all_entities:\n",
    "                ent_hedges = []\n",
    "                \n",
    "                #Assume the last word in the entity name is the fundamental one\n",
    "                ent_tokens = [token for token in doc if token.text == entity.name.split()[-1]]\n",
    "                \n",
    "                #If you format naming correctly (NO parentheses, should always be a match to the text), you shouldn't need this caveat\n",
    "                if len(ent_tokens) > 0:\n",
    "                    ent_token = ent_tokens[0]\n",
    "                \n",
    "                    for hedge in hedges:\n",
    "                        anc_hedges = list(set([hedge for anc in ent_token.ancestors if anc.text in hedge] + [hedge for child in ent_token.children if child.text in hedge]))\n",
    "                        for anc_hedge in anc_hedges:\n",
    "                            if anc_hedge not in self.__post_hedges:\n",
    "                                if self.__clause.index(anc_hedge) < self.__clause.index(ent_token.text):\n",
    "                                    ent_hedges.append(anc_hedge)\n",
    "                            else:\n",
    "                                ent_hedges.append(anc_hedge)  #because post hedges often can also be prior\n",
    "\n",
    "\n",
    "                    #2nd Layer Search for hedge among children\n",
    "                    if len(ent_hedges) == 0:\n",
    "                        #print(\"2nd layer search\")\n",
    "                        for hedge in hedges:\n",
    "                            child_hedges = [hedge for child in ent_token.children if child.text in hedge]\n",
    "                            #print(\"Child hedges:\", child_hedges)\n",
    "                            for child_hedge in child_hedges:\n",
    "                                if self.__clause.index(child_hedge) < self.__clause.index(ent_token.text):\n",
    "                                    ent_hedges.append(child_hedge)\n",
    "                    \n",
    "                    ent_hedges = list(set(ent_hedges))\n",
    "                    if len(ent_hedges) > 0:\n",
    "                        entity.set_hedging(self.remove_submatches(ent_hedges))\n",
    "\n",
    "                    for descriptor in entity.description:\n",
    "                        descr_token = [token for token in doc if token.text == descriptor.name][0]\n",
    "                        descr_hedges = [anc.text for anc in descr_token.ancestors if anc.text in hedges] + [child.text for child in descr_token.children if child.text in hedges]\n",
    "                        if len(descr_hedges) > 0:\n",
    "                            descriptor.set_hedging(self.remove_submatches(descr_hedges))\n",
    "        return None\n",
    "   \n",
    "    def remove_redundant_anatomy(self, df):\n",
    "        findings_to_remove, idxs_to_check, findings_to_check = [], [], []\n",
    "        \n",
    "        for i in range(len(df)):\n",
    "            if df.finding_types.loc[i] == 'anatomy':# or df.finding_types.loc[i] == 'visual_disease':\n",
    "                idxs_to_check.append(i)\n",
    "                findings_to_check.append(df.findings[i])  \n",
    "                \n",
    "                \n",
    "        for k in range(len(findings_to_check)): \n",
    "            finding = findings_to_check[k]\n",
    "            for j in range(len(df)):\n",
    "                if j not in idxs_to_check:\n",
    "                    ent = df.iloc[j]\n",
    "                    slim_finding = finding.split(\"(\")[0].strip()  #this counts on the parentheses being at the end\n",
    "                    if ent.locations in slim_finding or slim_finding in ent.locations:\n",
    "                        findings_to_remove.append(finding)     \n",
    "         \n",
    "        for finding in findings_to_remove:\n",
    "            df = df[df.findings != finding]\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def check_lost(self):\n",
    "        num_chunks = np.sum([len(chunk_list) for chunk_list in self.__chunks])\n",
    "        if num_chunks > len(self.__all_entities):\n",
    "            ent_adjs = [adj for entity in self.__all_entities for adj in entity.describe().split(\",\")]\n",
    "            ent_adjs = ent_adjs + [adj for entity in self.__all_entities for adj in entity.location.split(\",\")]\n",
    "            ent_adjs = list(set([word.strip(\"\\'\") for ent in ent_adjs for word in ent.split(\" \")]))\n",
    "            \n",
    "            adjs = [token for token in nlp(self.__clause) if token.pos_ == 'ADJ']\n",
    "            missing_adjs = []\n",
    "            for adj in adjs:\n",
    "                if adj.text not in ent_adjs:\n",
    "                    missing_adjs.append(adj)\n",
    "            \n",
    "            #print(ent_adjs, missing_adjs, adjs)\n",
    "            if len(missing_adjs) > 0:\n",
    "                return True, True\n",
    "            else:\n",
    "                return True, False\n",
    "        else:\n",
    "            return False, False\n",
    "        \n",
    "    def grab_bag(self):\n",
    "        #print(\"IN GRAB BAG\")\n",
    "        doc = nlp(self.__clause)\n",
    "        root_verb = [token for token in doc if token.dep_ == 'ROOT']\n",
    "        if len(root_verb) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            root_verb = root_verb[0]\n",
    "            root_adjs = [child.text for child in root_verb.children if child.pos_ == 'ADJ']\n",
    "            relations = [token.text for token in doc if root_verb.text in [anc.text for anc in token.ancestors] and token.pos_ == 'NOUN']\n",
    "            \n",
    "            adj_string = \", \".join(root_adjs)\n",
    "            new_outputs = []\n",
    "            \n",
    "            for relation in relations:\n",
    "                new_clause = \"the \" + adj_string + \" \" + relation\n",
    "                sub_extractor = ReportExtractor(clause = new_clause, neg_rules = self.__neg_rules, \n",
    "                                               prev_rules = self.__prev_rules,\n",
    "                                               \n",
    "                                               vis_dis_list = self.__vis_dis_list, anatomy_list = self.__anatomy_list,\n",
    "                                               procedure_list = self.__procedure_list, device_list = self.__device_list, \n",
    "                                               \n",
    "                                                locations_list = self.__location_list, descriptor_list = self.__descriptor_list,\n",
    "                                                normal_list = self.__normal_list,\n",
    "                                                \n",
    "                                                hedge_list = self.__hedge_list, post_hedge_list = self.__post_hedges, \n",
    "                                                hedge_dict = self.__hedge_dict, hedge_scores = self.__hedge_scores,\n",
    "                                               grab = False)\n",
    "                new_outputs.append(sub_extractor.run_extractor())\n",
    "            \n",
    "            if len(new_outputs) is not 0:\n",
    "                return pd.concat(new_outputs)\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "    def remove_nonsense_locations(self, locations):\n",
    "        new_locs = []\n",
    "        for location in locations:\n",
    "            doc = nlp(location)\n",
    "            pos = [token.pos_ for token in doc]\n",
    "            if 'NOUN' in pos:\n",
    "                new_locs.append(location)\n",
    "            elif 'ADJ' in pos:\n",
    "                new_locs.append(location)\n",
    "            else:\n",
    "                new_locs.append('')\n",
    "        return new_locs\n",
    "            \n",
    "    \n",
    "    def clean_output(self):\n",
    "        chunks_lost, adjs_lost = self.check_lost()\n",
    "        addl_df = None\n",
    "        if chunks_lost and adjs_lost and self.__grab:\n",
    "            addl_df = self.grab_bag()\n",
    "        \n",
    "        findings, finding_types, certainties, statuses, descriptors, locations, changes = [], [], [], [], [], [], []\n",
    "        \n",
    "        for entity in self.__all_entities:\n",
    "            #finding, finding_type, ceratinty, status, descriptors, locations, is_changed\n",
    "            findings.append(entity.name)\n",
    "            finding_types.append(entity.ent_type)\n",
    "            certainties.append(entity.set_hedge_level(self.__hedge_dict, self.__hedge_scores))\n",
    "            descriptors.append(entity.describe())\n",
    "            locations.append(entity.location)\n",
    "            changes.append(entity.is_changed(self.__change_list))\n",
    "            \n",
    "            if entity.negated and entity.previous:\n",
    "                statuses.append(\"negated, previous\")\n",
    "            elif entity.negated:\n",
    "                statuses.append(\"negated\")\n",
    "            elif entity.previous:\n",
    "                statuses.append(\"previous\")\n",
    "            else:\n",
    "                statuses.append(\"current\")\n",
    "                \n",
    "        locations = self.remove_nonsense_locations(locations)\n",
    "        \n",
    "        print(len(findings), len(finding_types), len(certainties), len(statuses), len(descriptors), len(locations), len(changes))\n",
    "        output_df = pd.DataFrame([findings, finding_types, certainties, statuses, descriptors, locations, changes],\n",
    "                           index = ['findings', 'finding_types', 'certainties', 'statuses', 'descriptors',\n",
    "                                   'locations', 'changed']).T\n",
    "        if addl_df is not None:\n",
    "            addl_df.index = range(len(addl_df))\n",
    "            addl_copy = addl_df.copy()\n",
    "            for i in range(len(addl_df)):\n",
    "                finding = addl_df.findings[i]\n",
    "                if finding in output_df.findings.tolist():\n",
    "                    addl_copy = addl_copy.drop(i)\n",
    "            addl_df = addl_copy\n",
    "            if addl_copy is not None:\n",
    "                output_df = pd.concat([output_df, addl_copy])\n",
    "            \n",
    "        output_df.index = range(len(output_df))\n",
    "        \n",
    "        #print(\"CONCAT DF: \", output_df)\n",
    "        if self.__grab:\n",
    "            output_df = self.remove_redundant_anatomy(output_df)\n",
    "            \n",
    "        return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Patient Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_double_errors(df):\n",
    "    if 'effusion' in df.findings.values and 'pleural effusion' in df.findings.values:\n",
    "        df = df[df.findings != 'effusion']\n",
    "    if 'effusions' in df.findings.values and 'pleural effusions' in df.findings.values:\n",
    "        df = df[df.findings != 'effusions']\n",
    "    if 'process' in df.findings.values and re.findall('\\S+\\s+process', \", \".join(df.findings.values)):\n",
    "        df = df[df.findings != 'process']\n",
    "    if 'processes' in df.findings.values and re.findall('\\S+\\s+processes', \", \".join(df.findings.values)):\n",
    "        df = df[df.findings != 'processes']\n",
    "    if 'disease' in df.findings.values and (re.findall('\\S+\\s+disease', \", \".join(df.findings.values))):\n",
    "        df = df[df.findings != 'disease']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_submatches(matches_to_search):\n",
    "    unique_matches = []\n",
    "\n",
    "    while len(matches_to_search) > 0:\n",
    "        match1 = max(matches_to_search, key = len)\n",
    "        related_matches = [match1]\n",
    "        matches_to_search.remove(match1)     \n",
    "        for match2 in matches_to_search:\n",
    "            if match2 in match1:\n",
    "                related_matches.append(match2)\n",
    "        unique_matches.append(max(related_matches, key = len))\n",
    "        for match in related_matches:\n",
    "            if match in matches_to_search:\n",
    "                matches_to_search.remove(match)              \n",
    "    return unique_matches\n",
    "\n",
    "def get_abnormals(df):\n",
    "    abnormals = []\n",
    "    for finding in df[df.statuses == 'current'].findings:\n",
    "        is_normal = False\n",
    "        for word in normal_list + [\"stable\", \"aerated\", \"aerated (well)\"]:\n",
    "            if word in finding.replace(\"(\",\"\").replace(\")\",\"\"):\n",
    "                is_normal = True\n",
    "        if not is_normal:\n",
    "            abnormals.append(finding)\n",
    "    \n",
    "    #include previous procedures\n",
    "    procedure_df = df[df.finding_types == 'procedure']\n",
    "    procedure_df = procedure_df[procedure_df.statuses == 'previous']\n",
    "    for finding in procedure_df.findings:\n",
    "        abnormals.append(finding)\n",
    "    return list(set(remove_submatches(abnormals)))\n",
    "\n",
    "def get_abnormals_and_locs(df):\n",
    "    abnormal_locs = []\n",
    "    abnormal_descrs = []\n",
    "    \n",
    "    currents = df[df.statuses == 'current']\n",
    "    for idx in range(len(currents)):\n",
    "        finding = currents.iloc[idx].findings\n",
    "        is_normal = False\n",
    "        for word in normal_list + [\"stable\", \"aerated\", \"aerated (well)\"]:\n",
    "            if word in finding.replace(\"(\",\"\").replace(\")\",\"\"):\n",
    "                is_normal = True\n",
    "        if not is_normal:\n",
    "            abnormal_locs.append((finding, currents.iloc[idx].locations))\n",
    "            abnormal_descrs.append((finding, currents.iloc[idx].descriptors))\n",
    "    \n",
    "    #include previous procedures\n",
    "    procedure_df = df[df.finding_types == 'procedure']\n",
    "    procedure_df = procedure_df[procedure_df.statuses == 'previous']\n",
    "    for idx in range(len(procedure_df)):\n",
    "        finding = procedure_df.iloc[idx].findings\n",
    "        abnormal_locs.append((finding, procedure_df.iloc[idx].locations))\n",
    "        abnormal_descrs.append((finding, procedure_df.iloc[idx].descriptors))\n",
    "    return list(set(remove_submatches(abnormal_locs))), list(set(remove_submatches(abnormal_descrs)))\n",
    "\n",
    "def get_findings(df):\n",
    "    devices = []\n",
    "    df = df[df.statuses != 'negated']\n",
    "    df = df[df.statuses != 'negated, previous']\n",
    "    df = df[df.statuses != 'previous']\n",
    "    for finding in df.findings:\n",
    "        devices.append(finding)\n",
    "    return list(set(remove_submatches(devices)))\n",
    "\n",
    "def get_changes(df):\n",
    "    changes = []\n",
    "    df = df[df.statuses != 'negated']\n",
    "    df = df[df.statuses != 'negated, previous']\n",
    "    for i in range(len(df)):\n",
    "        change_description = df.iloc[i].findings + \", \" + df.iloc[i].descriptors\n",
    "        if len(change_description.split()) > 1:\n",
    "            changes.append(change_description)\n",
    "    return list(set(remove_submatches(changes)))\n",
    "\n",
    "def get_changes_and_locs(df):\n",
    "    change_locs = []\n",
    "    change_descrs = []\n",
    "    df = df[df.statuses != 'negated']\n",
    "    df = df[df.statuses != 'negated, previous']\n",
    "    for i in range(len(df)):\n",
    "        change_description = df.iloc[i].findings + \", \" + df.iloc[i].descriptors\n",
    "        if len(change_description.split()) > 1:\n",
    "            change_locs.append((change_description, df.iloc[i].locations))\n",
    "            change_descrs.append((df.iloc[i].findings, df.iloc[i].descriptors))\n",
    "    return list(set(remove_submatches(change_descrs))), list(set(remove_submatches(change_locs)))\n",
    "\n",
    "def convert_list_to_string(list_items):\n",
    "#     print(list_items)\n",
    "    if len(list_items)==0 or list_items is None:\n",
    "        final_str = \"\"\n",
    "    else:\n",
    "#         for idx, item in enumerate(list_items):\n",
    "#             if \",\" in str(item):\n",
    "#                 words = item.split(',')\n",
    "#                 item = \"__\".join([word.strip() for word in words])\n",
    "#                 list_items[idx] = item\n",
    "        final_str = \"--\".join([str(item) for item in list_items])\n",
    "    print(list_items, final_str)\n",
    "    return final_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #skipped 20737\n",
    "# def extract_from_reports(radnet):\n",
    "#     all_abnorms, all_changes = [], []\n",
    "#     for i in range(len(radnet)):\n",
    "#         abnormalities, changes = [], []\n",
    "#         if i % 1000 == 0:\n",
    "#             print(i)    \n",
    "#         if type(radnet.iloc[i].FINDINGS) != float and type(radnet.iloc[i].IMPRESSION) != float:\n",
    "#             patient_sents = pre_process_sents(radnet.iloc[i].FINDINGS + radnet.iloc[i].IMPRESSION)\n",
    "#             results = get_patient_results(patient_sents)\n",
    "#             if results is not None and len(results) > 0:\n",
    "#                 abnormalities = get_abnormals(results[results.finding_types != 'change'])\n",
    "#                 changes = get_changes(results[results.finding_types == 'change'])\n",
    "                \n",
    "#         elif type(radnet.iloc[i].FINDINGS) != float:\n",
    "#             patient_sents = pre_process_sents(radnet.iloc[i].FINDINGS)\n",
    "#             results = get_patient_results(patient_sents)\n",
    "#             if results is not None and len(results) > 0:\n",
    "#                 abnormalities = get_abnormals(results[results.finding_types != 'change'])\n",
    "#                 changes = get_changes(results[results.finding_types == 'change'])\n",
    "                \n",
    "#         elif type(radnet.iloc[i].IMPRESSION) != float:\n",
    "#             patient_sents = pre_process_sents(radnet.iloc[i].IMPRESSION)\n",
    "#             results = get_patient_results(patient_sents)\n",
    "#             if results is not None and len(results) > 0:\n",
    "#                 abnormalities = get_abnormals(results[results.finding_types != 'change'])\n",
    "#                 changes = get_changes(results[results.finding_types == 'change'])\n",
    "                \n",
    "#         all_abnorms.append(abnormalities)\n",
    "#         all_changes.append(changes)\n",
    "#     return pd.DataFrame([all_abnorms, all_changes], index = ['abnorms', 'changes']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#skipped 20737\n",
    "def extract_from_reports(radnet):\n",
    "    all_abnorms, all_devices, all_vis_diseases, all_changes, all_procedures, all_anatomies = [], [], [], [], [], []\n",
    "    for i in tqdm(range(len(radnet))):\n",
    "        abnormalities, devices, vis_diseases, changes, procedures, anatomies = [], [], [], [], [], []\n",
    "#         if i % 1000 == 0:\n",
    "#             print(i)\n",
    "        findings = radnet.iloc[i].FINDINGS\n",
    "        impression = radnet.iloc[i].IMPRESSION\n",
    "        conclusion = radnet.iloc[i].Conclusion\n",
    "        \n",
    "        report_interest = \"\"        \n",
    "        if type(findings) != float:\n",
    "            report_interest += findings        \n",
    "        if type(impression) != float:\n",
    "            report_interest += impression\n",
    "        if type(conclusion) != float:\n",
    "            report_interest += conclusion\n",
    "\n",
    "        patient_sents = pre_process_sents(report_interest)\n",
    "        results = get_patient_results(patient_sents)        \n",
    "        if results is not None and len(results) > 0:\n",
    "            abnormalities = get_abnormals(results[results.finding_types != 'change'])\n",
    "            \n",
    "#             devices = get_findings(results[results.finding_types == 'device'])\n",
    "#             vis_diseases = get_findings(results[results.finding_types == 'visual_disease'])\n",
    "            changes = get_changes(results[results.finding_types == 'change'])\n",
    "#             anatomies = get_findings(results[results.finding_types == 'anatomy'])\n",
    "            procedures = get_findings(results[results.finding_types == 'procedure'])\n",
    "\n",
    "        all_abnorms.append(abnormalities)\n",
    "        all_devices.append(devices)\n",
    "        all_anatomies.append(anatomies)\n",
    "        all_procedures.append(procedures)\n",
    "        all_vis_diseases.append(vis_diseases)\n",
    "        all_changes.append(changes)\n",
    "\n",
    "    return pd.DataFrame([all_abnorms, all_devices, all_vis_diseases, all_anatomies, all_procedures, all_changes], \\\n",
    "                        index = ['abnorms', 'devices', 'vis_diseases', 'anatomies', 'procedures', 'changes']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_patient_results(sents):\n",
    "    patient_sent_dfs = []\n",
    "    for sent in sents:\n",
    "        pt_paths, pt_neg_paths, pt_changes, pt_devices, pt_procedures, pt_norm_anatomy = [], [], [], [], [], []\n",
    "        clauses, clause_outputs = split_by_clause(sent, term_rules), []\n",
    "        for clause in clauses:\n",
    "            start = time.time()\n",
    "            extractor = ReportExtractor(clause = clause, neg_rules = neg_rules, prev_rules = prev_rules, \n",
    "\n",
    "                                        vis_dis_list = vis_dis_list, anatomy_list = anatomy_list, \n",
    "                                        procedure_list = procedure_list, \n",
    "                                        device_list = device_list, change_list = change_list,\n",
    "\n",
    "                                        locations_list = locations, descriptor_list = sort_list(descriptors), \n",
    "                                        normal_list = normal_list,\n",
    "\n",
    "                                       hedge_list = hedge_list, post_hedge_list = post_hedge_list, \n",
    "                                       hedge_dict = hedge_dict, hedge_scores = hedge_scores, grab = True)\n",
    "\n",
    "#             temp_results = extractor.run_extractor()\n",
    "#             print(temp_results.shape, time.time() - start)\n",
    "#             print(extractor.run_extractor())\n",
    "            clause_output = extractor.run_extractor()\n",
    "            if clause_output.shape[0]\n",
    "            clause_outputs.append(extractor.run_extractor())\n",
    "            print(\"run_extractor: {:.2f}sec\".format(time.time() - start), clause_outputs[-1].shape)\n",
    "        print(len(clause_outputs))\n",
    "        if len(clause_outputs) != 0:\n",
    "            print(pd.concat(clause_outputs).shape)\n",
    "            patient_sent_dfs.append(pd.concat(clause_outputs))\n",
    "        \n",
    "#     if len(patient_sent_dfs) != 0:\n",
    "#         pt_df = remove_double_errors(pd.concat(patient_sent_dfs).drop_duplicates())\n",
    "#         return pt_df\n",
    "#     else:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_findings_from_reports(df):\n",
    "    all_results = pd.DataFrame()\n",
    "    for i in tqdm(range(len(df))):\n",
    "        accnum = df.iloc[i].accnum\n",
    "        findings = df.iloc[i].FINDINGS\n",
    "        impression = df.iloc[i].IMPRESSION\n",
    "        conclusion = df.iloc[i].Conclusion\n",
    "        \n",
    "        report_interest = \"\"        \n",
    "        if type(findings) != float:\n",
    "            report_interest += findings        \n",
    "        if type(impression) != float:\n",
    "            report_interest += impression\n",
    "        if type(conclusion) != float:\n",
    "            report_interest += conclusion\n",
    "\n",
    "        patient_sents = pre_process_sents(report_interest)\n",
    "        results = get_patient_results(patient_sents)\n",
    "#         results['accnum'] = accnum\n",
    "#         all_results = all_results.append(results, ignore_index=True)\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.11sec (0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.16sec (1, 7)\n",
      "2\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.03sec (0, 7)\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.19sec (0, 7)\n",
      "2\n",
      "(0, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.03sec (0, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.09sec (2, 7)\n",
      "2\n",
      "(2, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.03sec (0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "1 1 1 1 1 1 1\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.18sec (2, 7)\n",
      "2\n",
      "(2, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.05sec (0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.22sec (1, 7)\n",
      "2\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.13sec"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|         | 1/10 [00:01<00:11,  1.25s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.11sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.09sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.10sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.13sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.17sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "1 1 1 1 1 1 1\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.40sec (1, 7)\n",
      "1\n",
      "(1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 20%|        | 2/10 [00:02<00:09,  1.25s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.09sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.12sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.06sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.07sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.09sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.12sec (0, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.06sec (0, 7)\n",
      "2\n",
      "(0, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 30%|       | 3/10 [00:03<00:07,  1.04s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.15sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.15sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.07sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.06sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.14sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.12sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.11sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.08sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.02sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.07sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.02sec (0, 7)\n",
      "1\n",
      "(0, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 40%|      | 4/10 [00:04<00:06,  1.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.11sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.02sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.11sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.11sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.16sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.14sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "1 1 1 1 1 1 1\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.20sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.06sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|     | 5/10 [00:05<00:05,  1.08s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.40sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.10sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.14sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.09sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.22sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "1 1 1 1 1 1 1\n",
      "1 1 1 1 1 1 1\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.40sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.07sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.04sec (0, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.11sec (1, 7)\n",
      "2\n",
      "(1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 60%|    | 6/10 [00:06<00:04,  1.17s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.16sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.11sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.03sec (0, 7)\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.17sec (0, 7)\n",
      "2\n",
      "(0, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.13sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.11sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.05sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.14sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.15sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.14sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.08sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.14sec (0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.06sec (1, 7)\n",
      "2\n",
      "(1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 70%|   | 7/10 [00:08<00:03,  1.26s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.12sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.10sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.05sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.17sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.12sec (1, 7)\n",
      "1\n",
      "(1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 80%|  | 8/10 [00:08<00:02,  1.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.10sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.10sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.08sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.30sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.40sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.10sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.10sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.09sec (2, 7)\n",
      "1\n",
      "(2, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 90%| | 9/10 [00:10<00:01,  1.17s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.12sec (1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.02sec (0, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.07sec (1, 7)\n",
      "3\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.06sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.04sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.15sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.09sec (1, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.04sec (0, 7)\n",
      "2\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.05sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.07sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "1 1 1 1 1 1 1\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.16sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.06sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.04sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.14sec (1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.05sec (1, 7)\n",
      "2\n",
      "(2, 7)\n",
      "3 3 3 3 3 3 3\n",
      "run_extractor: 0.15sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.02sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.10sec (1, 7)\n",
      "1\n",
      "(1, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.07sec (1, 7)\n",
      "1\n",
      "(1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|| 10/10 [00:11<00:00,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.02sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "2 2 2 2 2 2 2\n",
      "run_extractor: 0.09sec (2, 7)\n",
      "1\n",
      "(2, 7)\n",
      "0 0 0 0 0 0 0\n",
      "run_extractor: 0.02sec (0, 7)\n",
      "1\n",
      "(0, 7)\n",
      "1 1 1 1 1 1 1\n",
      "run_extractor: 0.07sec (1, 7)\n",
      "1\n",
      "(1, 7)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['accnum' 'findings' 'finding_types' 'certainties' 'statuses'\\n 'descriptors' 'locations' 'changed'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-018110a8c6a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(extracted_findings.columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accnum'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'findings'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'finding_types'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'certainties'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'statuses'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'descriptors'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'locations'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'changed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mextracted_findings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextracted_findings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2722\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2723\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['accnum' 'findings' 'finding_types' 'certainties' 'statuses'\\n 'descriptors' 'locations' 'changed'] not in index\""
     ]
    }
   ],
   "source": [
    "extracted_findings = extract_findings_from_reports(df[:10])\n",
    "# print(extracted_findings.columns)\n",
    "list_columns = ['accnum','findings','finding_types','certainties','statuses','descriptors','locations','changed']\n",
    "extracted_findings = extracted_findings[list_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['accnum', 'findings', 'finding_types', 'certainties', 'statuses',\n",
       "       'descriptors', 'locations', 'changed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_findings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Lungs: The lungs are well aerated. There are no infiltrates or nodules. Heart and aorta: The heart is in normal in position, size, contour, and x-ray attenuation. Bones: The bones around the margins of the chest are unremarkable. Hila: The hila are normal. There are no abnormal masses in the hila. Trachea and main bronchi: The trachea and main bronchi are normal in appearance. Soft tissues: The soft tissues around the chest margin are normal in appearance. Other:'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].FINDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "work_df = df[10000:20000]\n",
    "# work_df = df.sample(10, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10000/10000 [2:14:09<00:00,  1.50it/s] \n"
     ]
    }
   ],
   "source": [
    "list_columns = ['accnum', 'abnorms', 'devices', 'vis_diseases', 'anatomies', 'procedures', 'changes']\n",
    "extracted_findings = extract_from_reports(work_df)\n",
    "extracted_findings['accnum'] = pd.Series(list(work_df['accnum']))\n",
    "extracted_findings = extracted_findings[list_columns]\n",
    "\n",
    "extracted_findings['FINDINGS'] = pd.Series(list(work_df['FINDINGS']))\n",
    "extracted_findings['IMPRESSION'] = pd.Series(list(work_df['IMPRESSION']))\n",
    "extracted_findings['CONCLUSION'] = pd.Series(list(work_df['Conclusion']))\n",
    "extracted_findings['REPORT'] = pd.Series(list(work_df['report']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save Extracts\n",
    "extracted_findings.to_csv(\"outputs/extracts_all_1.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracts = pd.read_csv(\"outputs/extracts_all_1.csv\")#.drop(\"Unnamed: 0\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 11)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
