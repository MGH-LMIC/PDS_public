{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main function call is towards the bottom but you need to run all cells first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "\n",
    "Right now, this is set up to run on a very large list. If you have a shorter list of targets,\n",
    "you can make the code run faster by giving it shorter dictionaries. \n",
    "\n",
    "If you want to just look for a short list of findings, the easiest thing to do is fill in the vis_dis and \n",
    "anatomy lists (if an anatomical pathology is part of your finding list, do NOT put it in vis_dis because anatomy \n",
    "needs to be handled differently ) and set the other lists to []."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Run save_dicts() every time you make a change to your dictionaries and want to update them.  '''\n",
    "def save_dicts():\n",
    "    pd.DataFrame(vis_dis_list, columns = ['name']).to_csv(\"data/csvs/radlex_vis_dis.csv\")\n",
    "    pd.DataFrame(anatomy_list, columns = ['name']).to_csv(\"data/csvs/radlex_anatomy.csv\")\n",
    "    pd.DataFrame(device_list, columns = ['name']).to_csv(\"data/csvs/radlex_devices.csv\")\n",
    "    pd.DataFrame(procedure_list, columns = ['name']).to_csv(\"data/csvs/radlex_procedures.csv\")\n",
    "    pd.DataFrame(descriptors, columns = ['name']).to_csv(\"data/csvs/radlex_descriptor_list.csv\")\n",
    "\n",
    "    pd.DataFrame(change_list_narrow, columns = ['name']).to_csv('data/csvs/narrow_change_list.csv')\n",
    "    pd.DataFrame(normal_list, columns = ['name']).to_csv('data/csvs/normal_list.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dis_list = pd.read_csv(\"rules/radlex_vis_dis.csv\").name.tolist()\n",
    "anatomy_list = pd.read_csv(\"rules/radlex_anatomy.csv\").name.tolist()\n",
    "device_list = pd.read_csv(\"rules/radlex_devices.csv\").name.tolist()\n",
    "procedure_list = pd.read_csv('rules/radlex_procedures.csv').name.tolist()\n",
    "# change_list_narrow = pd.read_csv('rules/narrow_change_list.csv').name.tolist()\n",
    "normal_list = pd.read_csv('rules/normal_list.csv').name.tolist()\n",
    "descriptors = pd.read_csv(\"rules/radlex_descriptor_list.csv\").name.tolist()\n",
    "\n",
    "with open (\"rules/term_rules.p\", 'rb') as f:\n",
    "    term_rules = pickle.load(f)\n",
    "with open(\"rules/neg_rules.p\", \"rb\") as f:\n",
    "    neg_rules = pickle.load(f)\n",
    "with open(\"rules/prev_rules.p\", \"rb\") as f:\n",
    "    prev_rules = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "radnet = pd.read_excel(\"data/csvs/radnet_norm_parsed.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Classes (Descriptor, ChangeEntity, ExtractedEntity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''ExtractedEntity class:\n",
    "An \"entity\" refers to a finding of one of the 5 categories: 'Visual_Disease', 'Anatomy', 'Procedure', \n",
    "'Device', and 'Change' (see detailed comments at top of ReportExtractor class).\n",
    "\n",
    "An ExtractedEntity object allows us to store details about the finding. Here it's implemented to store the\n",
    "finding cateogry and it's previous & negation statuses. In more advanced version of the code, could be modified\n",
    "to include hedging, location, descriptive terms etc. \n",
    "'''\n",
    "\n",
    "class ExtractedEntity(object):\n",
    "    def __init__(self, name, ent_type = '', descriptors = [], is_previous = False, is_normal = False, \n",
    "                 is_negated = False, is_changed = False):\n",
    "        self.name = name\n",
    "        self.ent_type = ent_type  #'visual_disease', 'anatomy', 'procedure', 'device', or 'change'\n",
    "        self.descriptors = descriptors #List of Descriptor objects\n",
    "        self.previous = is_previous  #Boolean\n",
    "        self.negated = is_negated  #Boolean\n",
    "        self.is_changed = is_changed\n",
    "    \n",
    "    def describe(self):\n",
    "        if len(self.descriptors) > 0:\n",
    "            return \", \".join([descr.describe() for descr in self.descriptors])\n",
    "        return \"\"\n",
    "    \n",
    "        \n",
    "'''Descriptor class: an object holding one or several descriptive terms attributed to a given entity \n",
    "Qualifiers represents an additional terms that describe the Descriptor \n",
    "(ie, in the phrase \"mild degenerative changes\", \"change\" is the basic entity, \n",
    "degenerative\" is the descriptor, mild\" is a qualifier)\n",
    "'''\n",
    "class Descriptor(object):\n",
    "    def __init__(self, name, qualifiers = []):\n",
    "        self.name = name\n",
    "        self.qualifiers = qualifiers  \n",
    "    \n",
    "    def describe(self):\n",
    "        if len(self.qualifiers) > 0:\n",
    "            return self.name + \" (\" + \", \".join(self.qualifiers) + \")\"\n",
    "        else:\n",
    "            return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "CATEGORIES (INFORMATION STRUCTURING)\n",
    "The 5 Categories are called 'Visual_Disease', 'Anatomy', 'Procedure', 'Device', and 'Change'. \n",
    "\n",
    "An \"entity\" refers to a finding of one of these categories.\n",
    "\n",
    "Of these categories, 'Anatomy', 'Procedure', and 'Device' are self-explanatory. Note that the terms in the'Anatomy'\n",
    "category are ONLY the mentions of anatomy that are modified by at least one descriptive word (ie, \"tortuous aorta\"). \n",
    "This is meant to avoid extracting anatomy that is mentioned only to locate other findings (ie, \"infiltrate \n",
    "in left lung\"). \n",
    "\n",
    "The 'Change' category is meant to catch pathologic changes, such as \"degenerative changes of the spine\" or \n",
    "\"kyphotic changes.\" The output of a \"Change' entity will be a word referencing the sort of change (ie, \"change\",\n",
    "\"increasing\", etc.) and the details can be found in the Descriptor terms (ie, \"kyphotic\", \"degenerative\", etc.).\n",
    "\n",
    "The 'Visual_Disease' category can be thought of as the \"everything else\" category. It should include anything \n",
    "that can be visually observed on the chest X-ray that does not fit into the other 4 categories. \n",
    "\n",
    "USE OF TOKEN VS TERM IN THE VARIABLE NAMES\n",
    "Throughout this code, \"Token\" and \"Term\" are used for similar concepts. A \"term\" is the str() (plain text) version\n",
    "of a phrase or word. A \"token\" is the Spacy-ified version of the phrase/word, which means that it carries with it\n",
    "the grammatical information stored from Spacy's original parse of the clause.\n",
    "\n",
    "RULES FORMAT\n",
    "neg_rules (negation rules) and prev_rules (previous rules) are lists of lists. These are the basis for this algorithm's\n",
    "detection of previous and negated features. \n",
    "\n",
    "Each entry of these outer lists should be formatted as follows, similar to the original NegEx:\n",
    " ['phrase',\n",
    "  '',\n",
    "  '[TAG]',\n",
    "  re.compile(r'-regex-phrase\\b', re.IGNORECASE|re.UNICODE)],\n",
    " \n",
    "An example:\n",
    "['previous', '',\n",
    "  '[PREV]',\n",
    "  re.compile(r'\\b(previous)\\b', re.IGNORECASE|re.UNICODE)],\n",
    "\n",
    "There are 4 types of tags for the negation rules. [PREN] and [POSN] are for pre- and post-negations respectively. \n",
    "[PREP] and [POSP] are for *possibly* pre- or post-negations respectively.\n",
    "In this code, the [PREN]/[PREP] and [POSN]/[POSP] pairs are treated identically although the original NegEx code\n",
    "handled them differently, as uncertainty about a diagnosis will be accounted for by the hedging portion of the code\n",
    "(still to be implemented Nov 2018). \n",
    "\n",
    "There is only 1 tag for the previous rules, [PREV]. \n",
    "'''\n",
    "\n",
    "class ReportExtractor(object):\n",
    "    def __init__(self, \n",
    "                 clause = None, neg_rules = None, prev_rules = None, \n",
    "                 vis_dis_list = [], anatomy_list = [], procedure_list = [], device_list = [], \n",
    "                 change_list = [], descriptor_list = [], normal_list = []):\n",
    "        \n",
    "        self.__filler = '_'\n",
    "        self.__clause = clause\n",
    "        self.__clause_doc = nlp(clause)\n",
    "        self.__keys = ['visual_disease', 'anatomy', 'procedure', 'device', 'change']\n",
    "        self.__neg_rules = neg_rules  #ALLI SHOULD YOU BE SORTING THESE?\n",
    "        self.__prev_rules = prev_rules\n",
    "        self.__vis_dis_list = self.sort_list(vis_dis_list)\n",
    "        self.__anatomy_list = self.sort_list(anatomy_list)\n",
    "        self.__change_list = self.sort_list(change_list)\n",
    "        self.__procedure_list = self.sort_list(procedure_list)\n",
    "        self.__device_list = self.sort_list(device_list)\n",
    "        self.__normal_list = self.sort_list(normal_list)\n",
    "        self.__descriptor_list = self.sort_list(descriptor_list)\n",
    "        \n",
    "        self.__joined_rules = []\n",
    "        \n",
    "        for rule_set in [neg_rules, prev_rules]:\n",
    "            if rule_set is not None:\n",
    "                self.__joined_rules = self.__joined_rules + rule_set\n",
    "        \n",
    "        #Bind the lists of terms into one dictionary of the term lists \n",
    "        self.__term_dicts = self.bind_term_lists([vis_dis_list, anatomy_list, procedure_list, device_list, change_list])\n",
    "        #List the negation phrases for the change negation detector\n",
    "        self.__neg_list = [rule[0] for rule in neg_rules if rule[3] is not '[PSEU]']\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''MAIN CALL'''\n",
    "    def run_extractor(self):\n",
    "        #Get words/phrases from the clause that map to the term lists\n",
    "        self.__term_lists, self.__tagged_term_lists = self.run_term_finder()\n",
    "        #Tag the clause with the negation & previous rules\n",
    "        self.__tagged_clause = self.tag(self.__clause)\n",
    "        #Get the indices of the mapped chunks \n",
    "        self.__term_idx_dicts = self.get_indices()\n",
    "        #Compare the indices to the negation/previous tags, find modifying terms, \n",
    "        # return a list of ExtractedEntity objects (nice packaging)\n",
    "        self.__all_entities = self.make_structured_entities()\n",
    "        #\n",
    "        return self.clean_output()\n",
    "\n",
    "    \n",
    "    '''INITIALIZING FUNCTIONS'''\n",
    "    '''Sort lists by descending length (most specific first) and exclude repeated elements'''\n",
    "    def sort_list(self, lst):\n",
    "        lengths = [len(item) for item in lst]\n",
    "        lst_df = pd.Series(lengths, index = lst) #, columns = ['len'])\n",
    "        lst_df = lst_df.sort_values(ascending = False)\n",
    "        return list(set(lst_df.index.tolist()))\n",
    "    \n",
    "    '''Create a dictionary of the category lists for access via indices'''\n",
    "    def bind_term_lists(self, lists):\n",
    "        term_list_dict, key_idx = {}, 0\n",
    "        for lst in lists:\n",
    "            term_list_dict[self.__keys[key_idx]] = lst\n",
    "            key_idx += 1\n",
    "        return term_list_dict\n",
    "    \n",
    "    '''Tag the given phrase according to all the rule sets'''\n",
    "    def tag(self, phrase):\n",
    "        for rule in self.__joined_rules:\n",
    "                reformatRule = re.sub(r'\\s+', self.__filler, rule[0].strip())\n",
    "                phrase = rule[3].sub(' ' + rule[2].strip() + reformatRule + rule[2].strip() + ' ', phrase)\n",
    "        return phrase \n",
    "    \n",
    "    \n",
    "    '''MAPPING TO RADLEX FUNCTIONS'''\n",
    "    '''Map text (one or many words) to the given term list by attempting common substitutions to handle\n",
    "    grammatical variations.  '''\n",
    "    def map_text_to_term_list(self, text, term_list):\n",
    "        text = text.strip()\n",
    "        if text in term_list:\n",
    "            return text\n",
    "        elif text[:-1] in term_list:\n",
    "            return text\n",
    "        else:  #Check a list of substituted phrases against the term_list\n",
    "            #Suffix substitutions\n",
    "            subs = [(\"'s\", \"\"), (\"ies\", \"y\"), (\"es\", \"is\"), (\"ing\", \"ed\"), (\"ing\", \"\"), (\"ring\", \"\"), \n",
    "                (\"ion\", \"ed\"), (\"ative\", \"ed\"), (\"tous\", \"\"), (\"ed\", \"ement\"), (\"iness\", \"y\"), (\"ly\", \"\"), \n",
    "                (\"ity\", \"\"), (\"e\", \"ion\"), \n",
    "                #Affix substitutions\n",
    "                (\"para\", \"\"), (\"peri\", \"\"), (\"bi\", \"\")]\n",
    "\n",
    "            for sub_tuple in subs:\n",
    "                if text.replace(sub_tuple[0], sub_tuple[1]) in term_list:\n",
    "                    return text\n",
    "        return \n",
    "        \n",
    "    \n",
    "    '''Map a potential term (potentially multiple words) to a given term list.'''\n",
    "    def map_term_to_term_list(self, text, term_list): \n",
    "        text_words, word_idx, found_mapping = text.split(), 0, False\n",
    "        \n",
    "        #Attempt to map as many words as possible to the term list\n",
    "        while not found_mapping and word_idx < len(text_words):\n",
    "            mapped_term = self.map_text_to_term_list(text, term_list) \n",
    "            if mapped_term is not None:\n",
    "                 found_mapping = True\n",
    "            else:  #If no match found, repeatedly remove the first word from the phrase and try again\n",
    "                word_idx += 1\n",
    "                text = \" \".join(text_words[word_idx:])  \n",
    "        \n",
    "        #If still no match found, try each individual word in the term\n",
    "        if word_idx == len(text_words) and not found_mapping:\n",
    "            for word in text_words[:-1]:\n",
    "                if not found_mapping:\n",
    "                    mapped_term = self.map_text_to_term_list(word, term_list) \n",
    "                    if mapped_term is not None:\n",
    "                        found_mapping = True           \n",
    "        return mapped_term\n",
    "    \n",
    "    '''Select the longest (therefore the most specific) versions of every term for which a positive match was found.'''\n",
    "    def remove_submatches(self, matches_to_search):\n",
    "        unique_matches = []\n",
    "        while len(matches_to_search) > 0:\n",
    "            match1 = max(matches_to_search, key = len)\n",
    "            related_matches = [match1]\n",
    "            matches_to_search.remove(match1)     \n",
    "            for match2 in matches_to_search:\n",
    "                if match2 in match1:\n",
    "                    related_matches.append(match2)\n",
    "            unique_matches.append(max(related_matches, key = len))\n",
    "            for match in related_matches:\n",
    "                if match in matches_to_search:\n",
    "                    matches_to_search.remove(match)              \n",
    "        return unique_matches\n",
    "    \n",
    "    '''Map possible anatomical modifiers to the descriptor list'''\n",
    "    def map_modifiers(self, possible_modifiers):\n",
    "        found_descriptors = []\n",
    "        for modifier in possible_modifiers:\n",
    "            description = self.map_term_to_term_list(modifier, self.__descriptor_list + self.__normal_list)\n",
    "            if description is not None:\n",
    "                descr_name, adverbs = description, []\n",
    "                descr_token = [token for token in self.__clause_doc if token.text == description][0]\n",
    "                #Find and save words that modify core descriptor words (for instance, \"mildly overinflated\")\n",
    "                if descr_token.pos_ == 'VERB' or descr_token.pos_ == 'ADJ':\n",
    "                    adverbs = [child.text for child in descr_token.children if child.pos_ == 'ADV']\n",
    "                found_descriptors.append(Descriptor(name = descr_name, qualifiers = adverbs))\n",
    "        return found_descriptors\n",
    "\n",
    "    \n",
    "    '''Find the \"chunks\" (any noun, adjective, verb, or noun phrase that maps to a term list) and stores both \n",
    "    their original and rule-tagged versions. The point of tagging the chunks with the negation/previous rules \n",
    "    is to allow for accurate index comparison later on. This is because comparison of the term indices with the \n",
    "    indices of the negated/previous phrases will determine whether or not a given phrase \n",
    "    applies to the term at hand) '''\n",
    "    def run_term_finder(self):\n",
    "        num_keys = len(self.__keys)  #KEYS ORDER: 'visual-disease', 'anatomy', 'procedure', 'device', 'change'\n",
    "        \n",
    "        #Create storage for the list of tokens & tagged \n",
    "        term_lists, tagged_term_lists = [[] for i in range(num_keys)], [[] for i in range(num_keys)]\n",
    "\n",
    "        #Pull out every noun chunk, verb, adjective, and noun from the clause\n",
    "        #\"Long token\" flags that they might be +1 words long\n",
    "        long_tokens = list(set([chunk for chunk in self.__clause_doc.noun_chunks] + [token for token in self.__clause_doc if token.pos_ == 'VERB' or token.pos_ == 'NOUN' or token.pos_ == 'ADJ']))\n",
    "        \n",
    "        #Narrow the list to the most specific versions of a term possible. \n",
    "        narrowed_terms = self.remove_submatches([token.text for token in long_tokens])\n",
    "        #Also store the Spacy-ified version (to preserve the grammar relations)\n",
    "        long_tokens = [long_token for long_token in long_tokens if long_token.text in narrowed_terms]\n",
    "        self.__spacy_tokens = long_tokens\n",
    "        \n",
    "        #Map tokens to every term list & store any successfully mapped_term chunsk. \n",
    "        for term_list_idx in range(num_keys):\n",
    "            for token in long_tokens:\n",
    "                mapped_term = self.map_term_to_term_list(token.text, self.__term_dicts[self.__keys[term_list_idx]])\n",
    "                if mapped_term is not None and mapped_term not in term_lists[term_list_idx]:\n",
    "                    term_lists[term_list_idx].append(mapped_term)\n",
    "            term_lists[term_list_idx] = self.remove_submatches(term_lists[term_list_idx])\n",
    "        \n",
    "        # For cases in which \"lymph node\" is in the list of potential \"anatomy\" terms, \n",
    "        # remove \"node\" from the vis_disease list (no other category cross-checking exists in this code)\n",
    "        # because it would likely increase the False Negative rate by incorrectly discarding terms\n",
    "        if \"lymph node\" in \" \".join(term_lists[1]):\n",
    "            replace_vis_dis_list = []\n",
    "            for term in term_lists[0]:  #vis_dis \n",
    "                if \"node\" not in term:\n",
    "                    replace_vis_dis_list.append(term)\n",
    "            term_lists[0] = replace_vis_dis_list\n",
    "        \n",
    "        #Tag the mapped_term_text of the tokens with rules (allow for proper indexing later) & store the \n",
    "        tagged_term_list_idx = 0\n",
    "        for term_list in term_lists:\n",
    "            for mapped_term in term_list:\n",
    "                tagged_term_lists[tagged_term_list_idx].append(self.tag(mapped_term))\n",
    "            tagged_term_list_idx += 1\n",
    "        \n",
    "        return term_lists, tagged_term_lists\n",
    "    \n",
    "    '''Get indices in the rule-tagged clause of every term in the list of tagged terms.\n",
    "    Return as a list of dictionaries with key-value pairs like [term: index in rule-tagged clause]'''\n",
    "    def get_indices(self):\n",
    "        term_idx_dicts = []\n",
    "        for lst in self.__tagged_term_lists:\n",
    "            term_idx_dict = {}\n",
    "            #Check to make sure doubled tags don't break the code\n",
    "            #(ie, \"no abnormal\" => [PREN]no abnormal[PREN] and [PREN]no[PREN])\n",
    "            for tagged_term in lst:\n",
    "                if tagged_term in self.__tagged_clause:  \n",
    "                    term_idx_dict[tagged_term] = self.__tagged_clause.index(tagged_term)\n",
    "                else:\n",
    "                    lst.remove(tagged_term)\n",
    "            term_idx_dicts.append(term_idx_dict)\n",
    "        return term_idx_dicts\n",
    "    \n",
    "\n",
    "    '''CHECK NEGATIONS AND PREVIOUS STATUS'''\n",
    "    \n",
    "    '''Check the negation status of a Change finding. This is separate from the other negation checker because\n",
    "    the occurring negation term needs to be connected directly to the change itself (raw_chunk) rather than\n",
    "    other nearby concepts'''\n",
    "    def check_change_negation(self, raw_chunk):\n",
    "        is_negated = False\n",
    "        \n",
    "        for token in self.__clause_doc:\n",
    "            if token.text in raw_chunk:\n",
    "                if token.pos_ == 'NOUN':\n",
    "                    to_check = [child.text for child in token.children]   \n",
    "                else:\n",
    "                    to_check = [child.text for anc in [anc for anc in token.ancestors] for child in anc.children]     \n",
    "                for word in self.__neg_list:\n",
    "                    if word in to_check:\n",
    "                        return True\n",
    "        return False\n",
    "        \n",
    "    '''Checks whether or not a term beginning at term_idx is negated. This is a modified form of the NegEx algorithm\n",
    "    (Chapman). '''\n",
    "    def check_negation(self, term_idx):\n",
    "        #Find the indices of the pre-negation, post-negation flags\n",
    "        clause_words, preneg_idxs, postneg_idxs = self.__tagged_clause.split(), [], []\n",
    "        \n",
    "        #Look for the negation flags in the tagged clause\n",
    "        #Assumes PREP and POSP are actually negations. \n",
    "        for word in clause_words:\n",
    "            if re.findall('\\[PREN\\]|\\[PREP\\]', word):\n",
    "                preneg_idxs.append(self.__tagged_clause.index(word))\n",
    "            if re.findall('\\[POST\\]|\\[POSP\\]', word):\n",
    "                postneg_idxs.append(self.__tagged_clause.index(word))\n",
    "              \n",
    "        #Return false if no negation tags present\n",
    "        if len(preneg_idxs) == 0 and len(postneg_idxs) == 0:\n",
    "            return False\n",
    "        \n",
    "        #Filter out the right pre/post negation tags with respect to the given term index\n",
    "        preneg_idxs = [neg_idx for neg_idx in preneg_idxs if neg_idx < term_idx]\n",
    "        postneg_idxs = [neg_idx for neg_idx in postneg_idxs if neg_idx > term_idx]\n",
    "        \n",
    "        #Set pre-neg/post-neg/previous indices, handling multiple negations\n",
    "        if len(preneg_idxs) % 2 == 0:\n",
    "            is_pre_negated = False\n",
    "        else:\n",
    "            is_pre_negated = True\n",
    "        \n",
    "        if len(postneg_idxs) % 2 == 0:\n",
    "            is_post_negated = False\n",
    "        else:\n",
    "            is_post_negated = True\n",
    "        neg_sum = is_pre_negated + is_post_negated\n",
    "        \n",
    "        if neg_sum % 2 == 0: #if not pre or post negated, or if both pre and post negated\n",
    "            return False\n",
    "        if neg_sum == 1:\n",
    "            return True \n",
    "    \n",
    "    '''Determine whether not a term beginning at the given term index is a previous condition or a current one.\n",
    "    This is an adaptation of the NegEx (Chapman) idea, similar to that proposed by PrevEx ##ALLI ADD CITATION). \n",
    "    It assumes that the earliest previous-phrase index (if multiple are found) will determine the \n",
    "    previous/current status of the term'''\n",
    "    def check_previous(self, term_idx):\n",
    "        clause_words, prev_idxs = self.__tagged_clause.split(), []\n",
    "        for word in clause_words:\n",
    "            if re.findall('\\[PREV\\]', word):\n",
    "                prev_idxs.append(self.__tagged_clause.index(word))\n",
    "        if len(prev_idxs) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            #Assume the earliest prev_idx determines the state of the term\n",
    "            prev_idx = prev_idxs[0]   \n",
    "            if prev_idx < term_idx:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    '''PARSING ENTITIES FUNCTIONS'''\n",
    "    \n",
    "    '''This function finds the modifiers of a given anatomical or change \"token\" (it's called token because\n",
    "    the term is still in the Spacy-ified form with the grammatical information intact )'''\n",
    "    def get_modifiers(self, token):\n",
    "        #Find all possible modifiers of a given token\n",
    "        children = [child.text for child in token.children if child.pos_ in ['ADJ', 'ADV', 'NOUN', 'VERB']]\n",
    "        ancestors = [anc.text for anc in token.ancestors if anc.pos_ in ['VERB','ADJ', 'ADV', 'NOUN']]\n",
    "        additionals = [tok.text for tok in self.__clause_doc if (tok.dep_ in ['conj', 'acomp', 'xcomp']) and (token in [child for child in tok.children] or token in [anc for anc in tok.ancestors])]\n",
    "        dets = [tok.text for tok in self.__clause_doc if tok.dep_ is 'det' and (token in [anc for anc in tok.ancestors])]\n",
    "        possible_modifiers = list(set(children + ancestors + additionals + dets))\n",
    "       \n",
    "        #Narrow the list of modifiers by checking that dependencies match from both directions\n",
    "        narrowed_modifiers = []\n",
    "        for modifier in possible_modifiers:\n",
    "            mod_token = [tok for tok in self.__clause_doc if tok.text == modifier][0]\n",
    "            mod_dependents = [child for child in mod_token.children] + [anc for anc in mod_token.ancestors]\n",
    "            if token in mod_dependents:\n",
    "                narrowed_modifiers.append(modifier)\n",
    "        return narrowed_modifiers\n",
    "    \n",
    "    \n",
    "    ''' Structure the entity at a given index into an ExtractedEntity object for easy handling.'''\n",
    "    def structure_entity(self, category_idx, ent_type):        \n",
    "        #A \"raw term\" is an untagged term\n",
    "        tagged_terms, raw_terms = self.__tagged_term_lists[category_idx], self.__term_lists[category_idx]\n",
    "        term_idx_dict = self.__term_idx_dicts[category_idx]\n",
    "        entity_list, term_list_idx  = [], 0\n",
    "        \n",
    "        while term_list_idx < len(tagged_terms):\n",
    "            tagged_term, raw_term = tagged_terms[term_list_idx], raw_terms[term_list_idx]\n",
    "            term_idx = term_idx_dict[tagged_term]\n",
    "            \n",
    "            #Get previous and negation statuses at this index in the clause\n",
    "            is_previous = self.check_previous(term_idx)\n",
    "            is_negated = self.check_negation(term_idx)\n",
    "            \n",
    "            #Find the modifiers of the term\n",
    "            possible_modifiers = []\n",
    "            for token in self.__clause_doc:   #Get all possible modifiers\n",
    "                if token.text in raw_term:\n",
    "                    possible_modifiers = possible_modifiers + self.get_modifiers(token)\n",
    "            #Map the modifiers to the descriptor term list. Return as a list of Descriptor objects\n",
    "            mod_descriptors = self.map_modifiers(list(set(possible_modifiers)))  \n",
    "\n",
    "            if not (ent_type == 'anatomy' and len(mod_descriptors) == 0): #Ignore anatomy words without descriptors\n",
    "                if ent_type == 'anatomy':  #Ensure that the Anatomy finding's name includes the descriptors, in parentheses\n",
    "                    ent_name = raw_term + \" (\" + \", \".join([descriptor.describe() for descriptor in mod_descriptors]) + \")\"\n",
    "                else: \n",
    "                    ent_name = raw_term\n",
    "                #Check the negation status of a change. ##ALLI WHY DOES THIS HAVE TO BE SEPARATE?\n",
    "                if ent_type == 'change':\n",
    "                    is_negated = self.check_change_negation(raw_term)\n",
    "                    \n",
    "                #Add the entity with its properties as an ExtractedEntity object to the list of all entities\n",
    "                entity_list.append(ExtractedEntity(name = ent_name, ent_type = ent_type,\n",
    "                                                    descriptors = mod_descriptors,\n",
    "                                                    is_previous = is_previous,\n",
    "                                                    is_negated = is_negated))\n",
    "            term_list_idx += 1\n",
    "        return entity_list\n",
    "    \n",
    "    '''Structure every entity from the term indices stored previously'''\n",
    "    def make_structured_entities(self):\n",
    "        return [entity for lst in [self.structure_entity(i, self.__keys[i]) for i in range(len(self.__keys))] for entity in lst]\n",
    "\n",
    "    \n",
    "    '''Remove repeats of pathologies that were mentioned multiple times with alternate names\n",
    "    (ie, avoid double counting in cases where a radiologist mentions \"pleural effusion\" in the Findings \n",
    "    and refers back to it as an \"effusion\" in the \"Impressions\" section)'''\n",
    "    def remove_double_mentions(self, df):\n",
    "        if 'effusion' in df.findings.values and 'pleural effusion' in df.findings.values:\n",
    "            df = df[df.findings != 'effusion']\n",
    "        if 'effusions' in df.findings.values and 'pleural effusions' in df.findings.values:\n",
    "            df = df[df.findings != 'effusions']\n",
    "        if 'process' in df.findings.values and re.findall('\\S+\\s+process', \", \".join(df.findings.values)):\n",
    "            df = df[df.findings != 'process']\n",
    "        if 'processes' in df.findings.values and re.findall('\\S+\\s+processes', \", \".join(df.findings.values)):\n",
    "            df = df[df.findings != 'processes']\n",
    "        if 'disease' in df.findings.values and (re.findall('\\S+\\s+disease', \", \".join(df.findings.values))):\n",
    "            df = df[df.findings != 'disease']\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    '''Handling exceptions that derive from overlapping dictionaries/lists'''\n",
    "    def clean_entity_exceptions(self):\n",
    "        for entity in self.__all_entities:\n",
    "            if entity.ent_type == 'anatomy' and 'pleural' in entity.name:\n",
    "                self.__all_entities.remove(entity)\n",
    "        return \n",
    "    \n",
    "    '''Format the Entities into a dataframe.'''\n",
    "    def clean_output(self):  \n",
    "        findings, finding_types, statuses, descriptors, changes, normal_flags = [], [], [], [], [], []\n",
    "        \n",
    "        self.clean_entity_exceptions()\n",
    "        for entity in self.__all_entities:\n",
    "            #finding, finding_type, descriptors, changes\n",
    "            findings.append(entity.name)\n",
    "            finding_types.append(entity.ent_type)\n",
    "            if entity.ent_type == 'anatomy' or entity.ent_type == 'change':\n",
    "                descriptors.append(entity.describe())\n",
    "            else:\n",
    "                descriptors.append(\"\")\n",
    "            changes.append(entity.is_changed)\n",
    "            \n",
    "            #Set status (ie, negated/previous/current condition)\n",
    "            if entity.negated and entity.previous:\n",
    "                statuses.append(\"negated, previous\")\n",
    "            elif entity.negated:\n",
    "                statuses.append(\"negated\")\n",
    "            elif entity.previous:\n",
    "                statuses.append(\"previous\")\n",
    "            else:\n",
    "                statuses.append(\"current\")\n",
    "                \n",
    "            #Potentially flag anatomy entities as \"normal\" (aka unproblematic) \n",
    "            #based on what terms are included in the \"Descriptors\" (ie, \"well-aerated lungs\")\n",
    "            #to allow for convenient exclusion later on\n",
    "            is_normal = False\n",
    "            if entity.ent_type == 'anatomy':\n",
    "                is_normal = False\n",
    "                norm_idx, end_norm_idx = 0, len(self.__normal_list)\n",
    "                while not is_normal and norm_idx < end_norm_idx:\n",
    "                    for ent in entity.describe().split(\",\"):\n",
    "                        if self.__normal_list[norm_idx] == ent:  #remove parentheses on descriptor terms\n",
    "                            is_normal = True\n",
    "                    norm_idx += 1\n",
    "            normal_flags.append(is_normal)\n",
    "                \n",
    "        output_df = pd.DataFrame([findings, finding_types, statuses, descriptors, changes, normal_flags],\n",
    "                           index = ['findings', 'finding_types', 'statuses', 'descriptors','changed', 'normal_flag']).T       \n",
    "        output_df = self.remove_double_mentions(output_df)\n",
    "            \n",
    "        output_df.index = range(len(output_df))\n",
    "        return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Pre-Processing and Clause Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process_sents(findings):\n",
    "    if type(findings) == None or type(findings) == float:\n",
    "        return []\n",
    "    else:\n",
    "        sentences = nltk.tokenize.sent_tokenize(findings)\n",
    "        sentences = [sent.lower() for sent in sentences]\n",
    "        sentences = [sent.split(\"   \") for sent in sentences]\n",
    "        sentences = [sent for sents in sentences for sent in sents]\n",
    "        sentences = [re.sub('\\d+?/\\d+?/\\d{2,}', '', sent) for sent in sentences]\n",
    "        sentences = [sent.replace(\"/\", \" \").replace(\"\\n\", \" \") for sent in sentences]\n",
    "        #Modify for abbreviations particular to your dataset as needed \n",
    "        sentences = [sent.replace(\"chronic obstructive pulmonary disease\", \"copd\") for sent in sentences]\n",
    "        sentences = [sent.replace(\"coronary artery bypass graft\", \"cabg\") for sent in sentences]\n",
    "        sentences = [sent.replace(\"coronary bypass surgery\", \"cabg\") for sent in sentences]\n",
    "        sentences = [sent.replace(\"tb\", \"tuberculosis\") for sent in sentences]\n",
    "        sentences = [sent.replace(\"cp\", \"costophrenic\") for sent in sentences]\n",
    "        sentences = [sent.replace(\".\", \" \") for sent in sentences]\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Split initial list of sentences into a list of sentences by clause'''\n",
    "def split_by_clause(sentence, term_rules):\n",
    "    \n",
    "    '''Subfunction to split up sentence if the word 'AND' is present'''\n",
    "    def split_ands(phrases):\n",
    "        new_phrases = []\n",
    "        for phrase in phrases:\n",
    "            if phrase.count('and') == 1 and \",\" not in phrase:\n",
    "                parts = phrase.split('and')\n",
    "                pos1, pos2 = [token.pos_ for token in nlp(parts[0])], [token.pos_ for token in nlp(parts[1])]\n",
    "                if 'NOUN' in pos1 and 'VERB' in pos1 and 'NOUN' in pos2 and 'VERB' in pos2:  #maybe also 'ADV'\n",
    "                    new_phrases.append(parts[0])\n",
    "                    new_phrases.append(parts[1])\n",
    "                else:\n",
    "                    new_phrases.append(phrase)\n",
    "            else:\n",
    "                new_phrases.append(phrase)\n",
    "        return new_phrases\n",
    "    \n",
    "    '''Subfunction to split up sentence into comma-separated phrases while not splitting lists'''\n",
    "    def split_sent_by_comma(sent):\n",
    "        list_start, list_end, comma_indices = 0, 0, [c.start() for c in re.finditer(',', sent)]\n",
    "\n",
    "        #if oxford comma\n",
    "        if re.findall(', (((\\w+\\s?){1,2},)\\s)+?(and|or)', sent):\n",
    "            lst_indices = [(c.start(), c.end()) for c in re.finditer(', (((\\w+\\s?){1,2},)\\s)+?(and|or)', sent)]\n",
    "        #if no oxford comma\n",
    "        elif re.findall('((\\w+\\s?){1,2},\\s?)+?(\\s\\w+)+?\\s(and|or)', sent):\n",
    "            lst_indices = [(c.start(), c.end()) for c in re.finditer('((\\w+\\s?){1,2},\\s?)+?(\\s\\w+)+?\\s(and|or)', sent)]\n",
    "        else:\n",
    "            lst_indices = []\n",
    "            \n",
    "        split_zones = [0]\n",
    "        for j in range(len(lst_indices)):\n",
    "            split_zones = split_zones + [i for i in range(split_zones[-1], lst_indices[j][0])] + [lst_indices[j][1]]\n",
    "        split_zones = split_zones + [i for i in range(split_zones[-1], len(sent))]\n",
    "        \n",
    "        to_split = [idx for idx in comma_indices if idx in split_zones]\n",
    "        sxns = [sent[i:j] for i, j in zip([0] + to_split, to_split + [len(sent)])]\n",
    "\n",
    "        return sxns\n",
    "\n",
    "    '''Subfunction with common features to apply to split clauses'''\n",
    "    def apply_split_rules(phrases):\n",
    "        phrases = [re.split(';|:', phrase) for phrase in phrases]\n",
    "        phrases = [phrase for sub_phrase in phrases for phrase in sub_phrase]\n",
    "        phrases = [phrase.split(\"  \") for phrase in phrases]\n",
    "        phrases = [phrase for sub_phrase in phrases for phrase in sub_phrase]\n",
    "        return split_ands(phrases)\n",
    "    \n",
    "    #NegEx-style detection of clause termination words. \n",
    "    term_pat, clauses = \"\\[TERM\\]\", []\n",
    "    \n",
    "    for rule in term_rules:  \n",
    "        #Tag sentences that contain any clause termination words\n",
    "        reformatRule = re.sub(r'\\s+', '_', rule[0].strip())\n",
    "        sentence = rule[3].sub(' ' + rule[2].strip() + reformatRule + rule[2].strip() + ' ', sentence)\n",
    "        \n",
    "    #If a clause termination word is present, split the phrases there & apply other splitting rules\n",
    "    if re.findall(term_pat, sentence, flags = re.IGNORECASE):   \n",
    "        phrases = re.split(term_pat, sentence, flags = re.IGNORECASE)\n",
    "        phrases = [\" \".join([word.strip() for word in phrase.split()]) for phrase in phrases if len(phrase.split()) > 1]\n",
    "        phrases = [split_sent_by_comma(phrase) for phrase in phrases]\n",
    "        phrases = apply_split_rules([phrase for sub_phrase in phrases for phrase in sub_phrase])\n",
    "        \n",
    "    #If no clause termination words are present, apply general splitting rules\n",
    "    else:  \n",
    "        phrases = apply_split_rules(split_sent_by_comma(sentence))\n",
    "    \n",
    "    #Return a list of clauses \n",
    "    return [phrase.lower() for phrase in phrases if len(phrase) != 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Wrapper For the Report Extractor (input = patient sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''This function will output a dataframe with every finding and all of its attributes generated from the \n",
    "given list of pre-processed sentences.\n",
    "\n",
    "Note that it doesn't take the ReportExtractor inputs (ie, lists or rules) as arguments.\n",
    "This would be easy to fix, but it's a lot to copy  so for clarity and ease of debugging, \n",
    "I'm assuming these lists have been defined under the same names earlier in the Jupyter field.\n",
    "If you want to change this, just copy all the ReportExtractor arguments into the function call behind sents.'''\n",
    "def get_patient_results(sents):\n",
    "    patient_sent_dfs = []\n",
    "    for sent in sents:\n",
    "        pt_paths, pt_neg_paths, pt_changes, pt_devices, pt_procedures, pt_norm_anatomy = [], [], [], [], [], []\n",
    "        clauses, clause_outputs = split_by_clause(sent, term_rules), []\n",
    "        for clause in clauses:\n",
    "            extractor = ReportExtractor(clause = clause, neg_rules = neg_rules, prev_rules = prev_rules, \n",
    "                                        vis_dis_list = vis_dis_list, anatomy_list = anatomy_list, \n",
    "                                        procedure_list = procedure_list, \n",
    "                                        device_list = device_list, change_list = change_list_narrow,\n",
    "                                        descriptor_list = sort_list(descriptors), \n",
    "                                        normal_list = normal_list)\n",
    "            clause_outputs.append(extractor.run_extractor())\n",
    "        if len(clause_outputs) != 0:\n",
    "            patient_sent_dfs.append(pd.concat(clause_outputs))\n",
    "        \n",
    "    if len(patient_sent_dfs) != 0:\n",
    "        return pd.concat(patient_sent_dfs).drop_duplicates()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReportExtractor Wrapper (CALL THIS)\n",
    "\n",
    "\n",
    "The default here is to look at the combined FINDINGS and IMPRESSION sections of the reports. It could be easily modified to pick up just one of these sections, or a section with another name (but be careful not to input any text in the past or future tense because the code will fail).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_from_reports(radnet):\n",
    "    all_abnorms, all_changes = [], []\n",
    "    for i in range(len(radnet)):\n",
    "        abnormalities, changes, text = [], [], None \n",
    "        \n",
    "        #Find the relevant section of the reports (modifiable)\n",
    "        if type(radnet.iloc[i].FINDINGS) != float and type(radnet.iloc[i].IMPRESSION) != float:\n",
    "            text = radnet.iloc[i].FINDINGS + radnet.iloc[i].IMPRESSION\n",
    "        elif type(radnet.iloc[i].FINDINGS) != float:\n",
    "            text = radnet.iloc[i].FINDINGS\n",
    "        elif type(radnet.iloc[i].IMPRESSION) != float:\n",
    "            text = radnet.iloc[i].IMPRESSION\n",
    "        \n",
    "        if text is not None:\n",
    "            patient_sents = pre_process_sents(text)\n",
    "            results = get_patient_results(patient_sents)\n",
    "            \n",
    "            #Pick whatever sub-batch of results you would like \n",
    "            #Right now, this picks up the 'current' findings without the normal flag \n",
    "            #ie, it excludes things like \"normal heart\"\n",
    "            if results is not None and len(results) > 0:\n",
    "                abnorms = results[results.statuses == 'current']\n",
    "                abnorms = abnorms[abnorms.normal_flag != True]\n",
    "                all_abnorms.append(abnorms.findings.tolist())\n",
    "            else:\n",
    "                all_abnorms.append([])\n",
    "    return pd.DataFrame([all_abnorms], index = ['abnorms']).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:406: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:408: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:410: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:412: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:414: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.325988054275513\n"
     ]
    }
   ],
   "source": [
    "#0.53 seconds per patient (old version: 0.96 seconds per patient)\n",
    "start = time.time()\n",
    "abnorm_df = extract_from_reports(radnet[:10])\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dicts()  #Save the dictionaries every time you want to update them"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (mammo)",
   "language": "python",
   "name": "mammo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
